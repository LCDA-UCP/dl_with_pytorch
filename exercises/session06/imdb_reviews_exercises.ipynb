{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc88e3cb2c788d8",
   "metadata": {},
   "source": [
    "# IMDB 50K Movie Reviews - Sentiment Classification with MLP\n",
    "\n",
    "## Overview\n",
    "\n",
    "### In the next exercises, you will work with the IMDB 50K Movie Reviews dataset to build a sentiment analysis model using a Multi-Layer Perceptron (MLP). You will practice essential steps in the data science pipeline such as data loading, preprocessing, feature generation, and training/testing a neural network model. This exercise should be completed using the `pandas`, `nltk`, `sklearn`, and `torch` libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967f805bf9ca917",
   "metadata": {},
   "source": [
    "### Exercise 1: Data Loading and Exploration\n",
    "**Objective**: Load the IMDB dataset and explore its structure.\n",
    "\n",
    "1. **Load the dataset** using `pandas`. The dataset is in the `data/` folder and the file name is `imdb_dataset.zip`. **Hint: you can load zip files with pandas by passing `compression='zip'` tp `pd.read_csv`**\n",
    "2. **Explore the dataset** by checking for missing values and getting a summary of the data. \n",
    "    - Check the shape of the dataset.\n",
    "    - Get the distribution of the sentiment labels (positive/negative reviews).\n",
    "3. Print the first few reviews and their corresponding labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53cce52163c4447",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-04 11:39:44,859\tINFO worker.py:1786 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows x 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import modin.pandas as pd\n",
    "df = pd.read_csv(\"../../data/imdb_dataset.zip\", compression='zip')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e57bb2bb70f9f4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6388a96dd14227db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `df.groupby(categorical_by, sort=False)` implementation has mismatches with pandas:\n",
      "the groupby keys will be sorted anyway, although the 'sort=False' was passed. See the following issue for more details: https://github.com/modin-project/modin/issues/3571.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "negative    25000\n",
       "positive    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a1d1bcc857961",
   "metadata": {},
   "source": [
    "### Exercise 2: Splitting the Data\n",
    "**Objective**: Split the data into training, validation and test sets.\n",
    "\n",
    "1. Split the dataset into features (reviews) and labels (sentiment).\n",
    "2. Use `train_test_split` from `sklearn` to split the dataset into training, validation and test sets (use an 60/20/20 split).\n",
    "3. Print the sizes of the training and test sets to ensure the splits were done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af8a536",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['review']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cd385320714117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7eebc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 30000\n",
      "Validation set size: 10000\n",
      "Test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set size: {len(x_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565a8072f03328a",
   "metadata": {},
   "source": [
    "### Exercise 3: Text Preprocessing\n",
    "**Objective**: Preprocess the text data to prepare it for feature generation.\n",
    "\n",
    "1. Lowercase the text data. **Hint: python has a built-in string method for this**.\n",
    "2. Remove any URL from the reviews. **Hint: you can use regular expressions for this**.\n",
    "3. Remove non-word and non-whitespace characters (punctuation, special characters, etc.). **Hint: you can use regular expressions for this**.\n",
    "4. Remove digits. **Hint: you can use regular expressions for this**.\n",
    "5. Tokenize the reviews into individual words. **Hint: you can use the `nltk` library for this**.\n",
    "6. Remove stopwords. **Hint: you can use the `nltk` library for this**.\n",
    "7. Perform stemming or lemmatization. **Hint: you can use the `nltk` library for this**.\n",
    "8. Apply the preprocessing steps to both the training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40632be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\diogo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\diogo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db16a820",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-word/non-whitespace characters\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    words = word_tokenize(text)  # Tokenize text\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stop words\n",
    "    words = [stemmer.stem(word) for word in words]  # Perform stemming\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2897b745",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = x_train.apply(preprocess_text)\n",
    "X_val = X_val.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2fd3f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18306    borrow slightli modifi titl comment say usual ...\n",
       "49528    product account movi also got voic work entir ...\n",
       "44745    far one worst movi ever seen life watch practi...\n",
       "46827    obvious inspir seen sometim even gruesom blood...\n",
       "27531    movi almost gener defin import us born earli p...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0dd5545db5d361",
   "metadata": {},
   "source": [
    "### Exercise 4: Feature Generation (TF-IDF)\n",
    "**Objective**: Convert the preprocessed text data into numerical features using TF-IDF.\n",
    "\n",
    "1. Use the `TfidfVectorizer` from `sklearn` to convert the reviews into numerical vectors.\n",
    "2. Limit the maximum number of features to 5,000 to reduce the dimensionality.\n",
    "3. Fit the vectorizer on the training set and transform both the training and test sets.\n",
    "4. Print the shape of the transformed feature sets to confirm the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0ab295c4db2b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 5000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf_matrix_train = tfidf.fit_transform(X_train)\n",
    "tfidf_matrix_valid = tfidf.transform(X_val)\n",
    "tfidf_matrix_test = tfidf.transform(X_test)\n",
    "\n",
    "tfidf_matrix_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61baaf1ab53478c0",
   "metadata": {},
   "source": [
    "### Exercise 5: Building the MLP Model (PyTorch)\n",
    "**Objective**: Build a simple Multi-Layer Perceptron (MLP) for binary classification.\n",
    "\n",
    "1. Define the MLP model using `torch.nn.Module`. The model should have:\n",
    "    - An input layer that matches the size of the TF-IDF features.\n",
    "    - Two hidden layers with ReLU activations.\n",
    "    - A single output layer with a sigmoid activation function.\n",
    "2. Print the model summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b8179e1ea1d73d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MLPClssifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPClssifier, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.fc1 =  torch.nn.Linear(input_dim, 50)    #  input layer (inp) -> hidden layer (hl1)\n",
    "        self.fc2 = torch.nn.Linear(50, 10)    #  hidden layer (hl1) -> hidden layer (hl2)\n",
    "        self.fc3 = torch.nn.Linear(10, 1)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return torch.nn.Sigmoid()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8773057adff1116a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClssifier(\n",
       "  (fc1): Linear(in_features=5000, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = MLPClssifier(tfidf_matrix_train.shape[1]).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9266420be193ab7",
   "metadata": {},
   "source": [
    "### Exercise 6: Training the Model\n",
    "**Objective**: Train the MLP model on the training data.\n",
    "\n",
    "1. Convert the TF-IDF feature matrices and labels into PyTorch tensors (the label needs to be binarized).\n",
    "2. Define the loss function (`BCELoss` for binary classification) and the optimizer (`Adam`).\n",
    "3. Implement a training loop to train the model for a specified number of epochs (e.g., 50).\n",
    "4. Monitor the training and validation loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3596094a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_map = y_train.map({'positive': 1, 'negative': 0})\n",
    "y_valid_map = y_val.map({'positive': 1, 'negative': 0})\n",
    "y_test_map = y_test.map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2698b469c554df8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataset.TensorDataset at 0x26104788d90>,\n",
       " <torch.utils.data.dataset.TensorDataset at 0x261135a03d0>,\n",
       " <torch.utils.data.dataset.TensorDataset at 0x2646a1cb7d0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "x_train_tfidf = torch.tensor(tfidf_matrix_train.toarray(), dtype=torch.float32).to(device)\n",
    "x_valid_tfidf = torch.tensor(tfidf_matrix_valid.toarray(), dtype=torch.float32).to(device)\n",
    "x_test_tfidf = torch.tensor(tfidf_matrix_test.toarray(), dtype=torch.float32).to(device)\n",
    "\n",
    "y_train = torch.tensor(y_train_map, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "y_valid = torch.tensor(y_valid_map, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "y_test = torch.tensor(y_test_map, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "\n",
    "train = TensorDataset(x_train_tfidf, y_train)\n",
    "valid = TensorDataset(x_valid_tfidf, y_valid)\n",
    "test = TensorDataset(x_test_tfidf, y_test)\n",
    "\n",
    "train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d316373ebf5c939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2527b1bffa065bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.6968812942504883\n",
      "Validation Loss: 0.6966938376426697\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 1, Loss: 0.6965698003768921\n",
      "Validation Loss: 0.6963880658149719\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 2, Loss: 0.6962490677833557\n",
      "Validation Loss: 0.6960293054580688\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 3, Loss: 0.695872962474823\n",
      "Validation Loss: 0.6956089735031128\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 4, Loss: 0.6954323053359985\n",
      "Validation Loss: 0.6951379776000977\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 5, Loss: 0.6949372291564941\n",
      "Validation Loss: 0.6946307420730591\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 6, Loss: 0.6944020986557007\n",
      "Validation Loss: 0.6940974593162537\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 7, Loss: 0.6938371658325195\n",
      "Validation Loss: 0.6935368180274963\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 8, Loss: 0.6932419538497925\n",
      "Validation Loss: 0.6929364800453186\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 9, Loss: 0.6926049590110779\n",
      "Validation Loss: 0.6922904253005981\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 10, Loss: 0.6919201016426086\n",
      "Validation Loss: 0.6916042566299438\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 11, Loss: 0.6911935210227966\n",
      "Validation Loss: 0.6908851265907288\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 12, Loss: 0.6904310584068298\n",
      "Validation Loss: 0.6901326775550842\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 13, Loss: 0.6896323561668396\n",
      "Validation Loss: 0.6893407106399536\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 14, Loss: 0.6887903213500977\n",
      "Validation Loss: 0.6885032653808594\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 15, Loss: 0.6878986358642578\n",
      "Validation Loss: 0.6876183152198792\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 16, Loss: 0.686954915523529\n",
      "Validation Loss: 0.6866863965988159\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 17, Loss: 0.685958743095398\n",
      "Validation Loss: 0.6857073307037354\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 18, Loss: 0.684910774230957\n",
      "Validation Loss: 0.6846803426742554\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 19, Loss: 0.6838100552558899\n",
      "Validation Loss: 0.6836022734642029\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 20, Loss: 0.6826531887054443\n",
      "Validation Loss: 0.6824673414230347\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 21, Loss: 0.6814359426498413\n",
      "Validation Loss: 0.6812711358070374\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 22, Loss: 0.680152952671051\n",
      "Validation Loss: 0.6800088286399841\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 23, Loss: 0.6787989139556885\n",
      "Validation Loss: 0.6786767244338989\n",
      "Accuracy: 0.4989999830722809\n",
      "Epoch: 24, Loss: 0.6773697137832642\n",
      "Validation Loss: 0.677272617816925\n",
      "Accuracy: 0.499099999666214\n",
      "Epoch: 25, Loss: 0.6758613586425781\n",
      "Validation Loss: 0.6757962107658386\n",
      "Accuracy: 0.4992999732494354\n",
      "Epoch: 26, Loss: 0.6742748618125916\n",
      "Validation Loss: 0.6742509007453918\n",
      "Accuracy: 0.5009999871253967\n",
      "Epoch: 27, Loss: 0.672613799571991\n",
      "Validation Loss: 0.6726383566856384\n",
      "Accuracy: 0.5072999596595764\n",
      "Epoch: 28, Loss: 0.6708803176879883\n",
      "Validation Loss: 0.6709579825401306\n",
      "Accuracy: 0.5180000066757202\n",
      "Epoch: 29, Loss: 0.6690735816955566\n",
      "Validation Loss: 0.6692079901695251\n",
      "Accuracy: 0.538100004196167\n",
      "Epoch: 30, Loss: 0.6671913862228394\n",
      "Validation Loss: 0.6673871278762817\n",
      "Accuracy: 0.5698999762535095\n",
      "Epoch: 31, Loss: 0.665231466293335\n",
      "Validation Loss: 0.6654917597770691\n",
      "Accuracy: 0.6121000051498413\n",
      "Epoch: 32, Loss: 0.6631902456283569\n",
      "Validation Loss: 0.6635187268257141\n",
      "Accuracy: 0.6491999626159668\n",
      "Epoch: 33, Loss: 0.6610642075538635\n",
      "Validation Loss: 0.661465048789978\n",
      "Accuracy: 0.6789000034332275\n",
      "Epoch: 34, Loss: 0.6588497757911682\n",
      "Validation Loss: 0.6593259572982788\n",
      "Accuracy: 0.7045999765396118\n",
      "Epoch: 35, Loss: 0.6565431356430054\n",
      "Validation Loss: 0.6570996046066284\n",
      "Accuracy: 0.7263000011444092\n",
      "Epoch: 36, Loss: 0.6541402339935303\n",
      "Validation Loss: 0.6547828912734985\n",
      "Accuracy: 0.7450999617576599\n",
      "Epoch: 37, Loss: 0.6516379117965698\n",
      "Validation Loss: 0.6523720622062683\n",
      "Accuracy: 0.7594000101089478\n",
      "Epoch: 38, Loss: 0.6490317583084106\n",
      "Validation Loss: 0.6498634815216064\n",
      "Accuracy: 0.770799994468689\n",
      "Epoch: 39, Loss: 0.6463180780410767\n",
      "Validation Loss: 0.6472547054290771\n",
      "Accuracy: 0.7824999690055847\n",
      "Epoch: 40, Loss: 0.6434937119483948\n",
      "Validation Loss: 0.6445432305335999\n",
      "Accuracy: 0.7926999926567078\n",
      "Epoch: 41, Loss: 0.640555739402771\n",
      "Validation Loss: 0.6417266130447388\n",
      "Accuracy: 0.7999999523162842\n",
      "Epoch: 42, Loss: 0.6375024318695068\n",
      "Validation Loss: 0.6388034820556641\n",
      "Accuracy: 0.8093000054359436\n",
      "Epoch: 43, Loss: 0.6343317627906799\n",
      "Validation Loss: 0.6357728242874146\n",
      "Accuracy: 0.8154000043869019\n",
      "Epoch: 44, Loss: 0.63104248046875\n",
      "Validation Loss: 0.6326330900192261\n",
      "Accuracy: 0.8212999701499939\n",
      "Epoch: 45, Loss: 0.6276327967643738\n",
      "Validation Loss: 0.6293827891349792\n",
      "Accuracy: 0.8241999745368958\n",
      "Epoch: 46, Loss: 0.6241012811660767\n",
      "Validation Loss: 0.6260208487510681\n",
      "Accuracy: 0.8281999826431274\n",
      "Epoch: 47, Loss: 0.620446503162384\n",
      "Validation Loss: 0.622546374797821\n",
      "Accuracy: 0.8313999772071838\n",
      "Epoch: 48, Loss: 0.6166672110557556\n",
      "Validation Loss: 0.6189586520195007\n",
      "Accuracy: 0.8346999883651733\n",
      "Epoch: 49, Loss: 0.6127626299858093\n",
      "Validation Loss: 0.6152576208114624\n",
      "Accuracy: 0.8367999792098999\n",
      "Epoch: 50, Loss: 0.6087324023246765\n",
      "Validation Loss: 0.6114434599876404\n",
      "Accuracy: 0.8381999731063843\n",
      "Epoch: 51, Loss: 0.604576826095581\n",
      "Validation Loss: 0.6075164079666138\n",
      "Accuracy: 0.8409000039100647\n",
      "Epoch: 52, Loss: 0.6002958416938782\n",
      "Validation Loss: 0.6034771203994751\n",
      "Accuracy: 0.8429999947547913\n",
      "Epoch: 53, Loss: 0.595890462398529\n",
      "Validation Loss: 0.5993272066116333\n",
      "Accuracy: 0.8447999954223633\n",
      "Epoch: 54, Loss: 0.5913615822792053\n",
      "Validation Loss: 0.5950679779052734\n",
      "Accuracy: 0.8460999727249146\n",
      "Epoch: 55, Loss: 0.5867106914520264\n",
      "Validation Loss: 0.5907009840011597\n",
      "Accuracy: 0.8466999530792236\n",
      "Epoch: 56, Loss: 0.5819398760795593\n",
      "Validation Loss: 0.5862292051315308\n",
      "Accuracy: 0.8476999998092651\n",
      "Epoch: 57, Loss: 0.5770514011383057\n",
      "Validation Loss: 0.5816553235054016\n",
      "Accuracy: 0.8482999801635742\n",
      "Epoch: 58, Loss: 0.5720480680465698\n",
      "Validation Loss: 0.5769824385643005\n",
      "Accuracy: 0.849399983882904\n",
      "Epoch: 59, Loss: 0.5669330954551697\n",
      "Validation Loss: 0.5722144842147827\n",
      "Accuracy: 0.849299967288971\n",
      "Epoch: 60, Loss: 0.5617101788520813\n",
      "Validation Loss: 0.5673559308052063\n",
      "Accuracy: 0.8499000072479248\n",
      "Epoch: 61, Loss: 0.5563834309577942\n",
      "Validation Loss: 0.5624111890792847\n",
      "Accuracy: 0.8503999710083008\n",
      "Epoch: 62, Loss: 0.5509575605392456\n",
      "Validation Loss: 0.5573849678039551\n",
      "Accuracy: 0.8508999943733215\n",
      "Epoch: 63, Loss: 0.5454378724098206\n",
      "Validation Loss: 0.5522828698158264\n",
      "Accuracy: 0.8514999747276306\n",
      "Epoch: 64, Loss: 0.5398294925689697\n",
      "Validation Loss: 0.5471106171607971\n",
      "Accuracy: 0.8521999716758728\n",
      "Epoch: 65, Loss: 0.5341382026672363\n",
      "Validation Loss: 0.5418747067451477\n",
      "Accuracy: 0.852400004863739\n",
      "Epoch: 66, Loss: 0.5283703804016113\n",
      "Validation Loss: 0.5365819931030273\n",
      "Accuracy: 0.852400004863739\n",
      "Epoch: 67, Loss: 0.5225326418876648\n",
      "Validation Loss: 0.53123939037323\n",
      "Accuracy: 0.8524999618530273\n",
      "Epoch: 68, Loss: 0.5166318416595459\n",
      "Validation Loss: 0.5258540511131287\n",
      "Accuracy: 0.8521999716758728\n",
      "Epoch: 69, Loss: 0.5106753706932068\n",
      "Validation Loss: 0.5204333066940308\n",
      "Accuracy: 0.8525999784469604\n",
      "Epoch: 70, Loss: 0.5046709179878235\n",
      "Validation Loss: 0.5149850249290466\n",
      "Accuracy: 0.852899968624115\n",
      "Epoch: 71, Loss: 0.498626172542572\n",
      "Validation Loss: 0.5095171928405762\n",
      "Accuracy: 0.8531999588012695\n",
      "Epoch: 72, Loss: 0.49254897236824036\n",
      "Validation Loss: 0.5040378570556641\n",
      "Accuracy: 0.8531999588012695\n",
      "Epoch: 73, Loss: 0.4864473044872284\n",
      "Validation Loss: 0.4985548257827759\n",
      "Accuracy: 0.8536999821662903\n",
      "Epoch: 74, Loss: 0.48032957315444946\n",
      "Validation Loss: 0.49307602643966675\n",
      "Accuracy: 0.8535999655723572\n",
      "Epoch: 75, Loss: 0.47420400381088257\n",
      "Validation Loss: 0.4876100420951843\n",
      "Accuracy: 0.854200005531311\n",
      "Epoch: 76, Loss: 0.4680795669555664\n",
      "Validation Loss: 0.4821655750274658\n",
      "Accuracy: 0.8542999625205994\n",
      "Epoch: 77, Loss: 0.46196413040161133\n",
      "Validation Loss: 0.47675058245658875\n",
      "Accuracy: 0.854699969291687\n",
      "Epoch: 78, Loss: 0.45586562156677246\n",
      "Validation Loss: 0.4713733196258545\n",
      "Accuracy: 0.8544999957084656\n",
      "Epoch: 79, Loss: 0.4497922956943512\n",
      "Validation Loss: 0.46604177355766296\n",
      "Accuracy: 0.8549999594688416\n",
      "Epoch: 80, Loss: 0.44375157356262207\n",
      "Validation Loss: 0.4607633054256439\n",
      "Accuracy: 0.8553999662399292\n",
      "Epoch: 81, Loss: 0.43775105476379395\n",
      "Validation Loss: 0.45554500818252563\n",
      "Accuracy: 0.85589998960495\n",
      "Epoch: 82, Loss: 0.4317980706691742\n",
      "Validation Loss: 0.45039448142051697\n",
      "Accuracy: 0.8565999865531921\n",
      "Epoch: 83, Loss: 0.42589977383613586\n",
      "Validation Loss: 0.44531795382499695\n",
      "Accuracy: 0.8567999601364136\n",
      "Epoch: 84, Loss: 0.42006275057792664\n",
      "Validation Loss: 0.44032207131385803\n",
      "Accuracy: 0.857699990272522\n",
      "Epoch: 85, Loss: 0.4142935276031494\n",
      "Validation Loss: 0.435412734746933\n",
      "Accuracy: 0.8578999638557434\n",
      "Epoch: 86, Loss: 0.4085981547832489\n",
      "Validation Loss: 0.43059518933296204\n",
      "Accuracy: 0.8587999939918518\n",
      "Epoch: 87, Loss: 0.40298205614089966\n",
      "Validation Loss: 0.4258742928504944\n",
      "Accuracy: 0.8592999577522278\n",
      "Epoch: 88, Loss: 0.39745032787323\n",
      "Validation Loss: 0.42125457525253296\n",
      "Accuracy: 0.8589999675750732\n",
      "Epoch: 89, Loss: 0.3920079469680786\n",
      "Validation Loss: 0.416740357875824\n",
      "Accuracy: 0.8596000075340271\n",
      "Epoch: 90, Loss: 0.38665884733200073\n",
      "Validation Loss: 0.4123355448246002\n",
      "Accuracy: 0.8597999811172485\n",
      "Epoch: 91, Loss: 0.3814065158367157\n",
      "Validation Loss: 0.40804338455200195\n",
      "Accuracy: 0.8597999811172485\n",
      "Epoch: 92, Loss: 0.3762544095516205\n",
      "Validation Loss: 0.4038667380809784\n",
      "Accuracy: 0.8598999977111816\n",
      "Epoch: 93, Loss: 0.37120532989501953\n",
      "Validation Loss: 0.39980757236480713\n",
      "Accuracy: 0.85999995470047\n",
      "Epoch: 94, Loss: 0.3662615418434143\n",
      "Validation Loss: 0.3958679139614105\n",
      "Accuracy: 0.8598999977111816\n",
      "Epoch: 95, Loss: 0.36142486333847046\n",
      "Validation Loss: 0.3920494019985199\n",
      "Accuracy: 0.8600999712944031\n",
      "Epoch: 96, Loss: 0.3566965162754059\n",
      "Validation Loss: 0.3883531093597412\n",
      "Accuracy: 0.8603999614715576\n",
      "Epoch: 97, Loss: 0.35207730531692505\n",
      "Validation Loss: 0.38477954268455505\n",
      "Accuracy: 0.8601999878883362\n",
      "Epoch: 98, Loss: 0.34756794571876526\n",
      "Validation Loss: 0.3813287615776062\n",
      "Accuracy: 0.8606999516487122\n",
      "Epoch: 99, Loss: 0.34316837787628174\n",
      "Validation Loss: 0.37800082564353943\n",
      "Accuracy: 0.8610000014305115\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = MLPClssifier(tfidf_matrix_train.shape[1]).to(device)\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 100\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "\n",
    "    outputs = model(x_train_tfidf)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(x_valid_tfidf)\n",
    "        val_loss = criterion(val_outputs, y_valid)\n",
    "        val_pred = (val_outputs >= 0.5).float()\n",
    "        accuracy = (val_pred.eq(y_valid)).sum() / y_valid.shape[0]\n",
    "        # accuracy2 = torch.sum(val_pred == y_valid) / y_valid.shape[0]\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "        print(f\"Validation Loss: {val_loss}\")\n",
    "        print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f0dec0e502aa8c",
   "metadata": {},
   "source": [
    "### Exercise 7: Model Evaluation\n",
    "**Objective**: Evaluate the performance of the trained model on the test data.\n",
    "\n",
    "1. Use the trained model to make predictions on the test set.\n",
    "2. Calculate the accuracy of the model on the test data.\n",
    "3. Print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d05d21ad2985d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8575999736785889"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(x_test_tfidf)\n",
    "    test_predictions = (test_outputs >= 0.5).float()\n",
    "    accuracy = torch.sum(test_predictions == y_test) / y_test.shape[0]\n",
    "\n",
    "accuracy.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a7f161b7501957",
   "metadata": {},
   "source": [
    "### **Exercise 8: Saving the Trained Model**\n",
    "\n",
    "1. Save the model's state_dict using `torch.save()`. \n",
    "\n",
    "2. Save the entire model, including its architecture and weights.\n",
    "\n",
    "3. Demonstrate how to load the saved model and use it for making predictions on new data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "349c9b2452e8c98f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:10:02.712871Z",
     "start_time": "2024-10-02T20:10:02.710085Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7e62e6f700facf51",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:10:02.757446Z",
     "start_time": "2024-10-02T20:10:02.755128Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('model.pt')\n",
    "model = MLPClssifier(tfidf_matrix_train.shape[1]).to(device)\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2043076079637bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model_full.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b7845ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClssifier(\n",
       "  (fc1): Linear(in_features=5000, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load('model_full.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

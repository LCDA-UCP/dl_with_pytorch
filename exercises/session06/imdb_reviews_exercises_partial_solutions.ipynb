{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc88e3cb2c788d8",
   "metadata": {},
   "source": [
    "# IMDB 50K Movie Reviews - Sentiment Classification with MLP\n",
    "\n",
    "## Overview\n",
    "\n",
    "### In the next exercises, you will work with the IMDB 50K Movie Reviews dataset to build a sentiment analysis model using a Multi-Layer Perceptron (MLP). You will practice essential steps in the data science pipeline such as data loading, preprocessing, feature generation, and training/testing a neural network model. This exercise should be completed using the `pandas`, `nltk`, `sklearn`, and `torch` libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967f805bf9ca917",
   "metadata": {},
   "source": [
    "### Exercise 1: Data Loading and Exploration\n",
    "**Objective**: Load the IMDB dataset and explore its structure.\n",
    "\n",
    "1. **Load the dataset** using `pandas`. The dataset is in the `data/` folder and the file name is `imdb_dataset.zip`. **Hint: you can load zip files with pandas by passing `compression='zip'` tp `pd.read_csv`**\n",
    "2. **Explore the dataset** by checking for missing values and getting a summary of the data. \n",
    "    - Check the shape of the dataset.\n",
    "    - Get the distribution of the sentiment labels (positive/negative reviews).\n",
    "3. Print the first few reviews and their corresponding labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a53cce52163c4447",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:57:50.262350Z",
     "start_time": "2024-10-04T08:57:43.060536Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: The size of /dev/shm is too small (7115059200 bytes). The required size at least half of RAM (7248187392 bytes). Please, delete files in /dev/shm or increase size of /dev/shm with --shm-size in Docker. Also, you can can override the memory size for each Ray worker (in bytes) to the MODIN_MEMORY environment variable.\n",
      "2024-10-04 09:57:47,183\tINFO worker.py:1786 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import modin.pandas as pd # Optimized distributed version of Pandas\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../../data/imdb_dataset.zip', compression='zip')  # Load as per your format\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e57bb2bb70f9f4ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:57:50.369072Z",
     "start_time": "2024-10-04T08:57:50.314739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6388a96dd14227db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:57:50.575362Z",
     "start_time": "2024-10-04T08:57:50.435532Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `df.groupby(categorical_by, sort=False)` implementation has mismatches with pandas:\n",
      "the groupby keys will be sorted anyway, although the 'sort=False' was passed. See the following issue for more details: https://github.com/modin-project/modin/issues/3571.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "negative    25000\n",
       "positive    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the distribution of labels\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a1d1bcc857961",
   "metadata": {},
   "source": [
    "### Exercise 2: Splitting the Data\n",
    "**Objective**: Split the data into training, validation and test sets.\n",
    "\n",
    "1. Split the dataset into features (reviews) and labels (sentiment).\n",
    "2. Use `train_test_split` from `sklearn` to split the dataset into training, validation and test sets (use an 60/20/20 split).\n",
    "3. Print the sizes of the training, validation and test sets to ensure the splits were done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cd385320714117c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:57:51.588285Z",
     "start_time": "2024-10-04T08:57:51.073728Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into features (X) and labels (y)\n",
    "X = df['review']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7543576eeb9d21ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:57:51.816654Z",
     "start_time": "2024-10-04T08:57:51.726569Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split into training (60%), validation (20%), and test (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb278e805672ee97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:57:51.843956Z",
     "start_time": "2024-10-04T08:57:51.836948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 30000\n",
      "Validation set size: 10000\n",
      "Test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565a8072f03328a",
   "metadata": {},
   "source": [
    "### Exercise 3: Text Preprocessing\n",
    "**Objective**: Preprocess the text data to prepare it for feature generation.\n",
    "\n",
    "1. Lowercase the text data. **Hint: python has a built-in string method for this**.\n",
    "2. Remove any URL from the reviews. **Hint: you can use regular expressions for this**.\n",
    "3. Remove non-word and non-whitespace characters (punctuation, special characters, etc.). **Hint: you can use regular expressions for this**.\n",
    "4. Remove digits. **Hint: you can use regular expressions for this**.\n",
    "5. Tokenize the reviews into individual words. **Hint: you can use the `nltk` library for this**.\n",
    "6. Remove stopwords. **Hint: you can use the `nltk` library for this**.\n",
    "7. Perform stemming or lemmatization. **Hint: you can use the `nltk` library for this**.\n",
    "8. Apply the preprocessing steps to both the training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "274001c7274aa06d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:57:53.278971Z",
     "start_time": "2024-10-04T08:57:52.866493Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/joao-\n",
      "[nltk_data]     correia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/joao-\n",
      "[nltk_data]     correia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20728d8e4d17ae5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:57:54.189040Z",
     "start_time": "2024-10-04T08:57:54.186192Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-word/non-whitespace characters\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    words = word_tokenize(text)  # Tokenize text\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stop words\n",
    "    words = [stemmer.stem(word) for word in words]  # Perform stemming\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "838d9aed37b5086b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:58:05.044099Z",
     "start_time": "2024-10-04T08:57:55.475388Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing to the training, validation, and test sets\n",
    "X_train = X_train.apply(preprocess_text)\n",
    "X_val = X_val.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "198fe3ef1a4716f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:58:08.428004Z",
     "start_time": "2024-10-04T08:58:08.417791Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18306    borrow slightli modifi titl comment say usual ...\n",
       "49528    product account movi also got voic work entir ...\n",
       "44745    far one worst movi ever seen life watch practi...\n",
       "46827    obvious inspir seen sometim even gruesom blood...\n",
       "27531    movi almost gener defin import us born earli p...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0dd5545db5d361",
   "metadata": {},
   "source": [
    "### Exercise 4: Feature Generation (TF-IDF)\n",
    "**Objective**: Convert the preprocessed text data into numerical features using TF-IDF.\n",
    "\n",
    "1. Use the `TfidfVectorizer` from `sklearn` to convert the reviews into numerical vectors.\n",
    "2. Limit the maximum number of features to 5,000 to reduce the dimensionality.\n",
    "3. Fit the vectorizer on the training set and transform both the training, validation and test sets.\n",
    "4. Print the shape of the transformed feature sets to confirm the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ab295c4db2b89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:58:09.421504Z",
     "start_time": "2024-10-04T08:58:09.419719Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61baaf1ab53478c0",
   "metadata": {},
   "source": [
    "### Exercise 5: Building the MLP Model (PyTorch)\n",
    "**Objective**: Build a simple Multi-Layer Perceptron (MLP) for binary classification.\n",
    "\n",
    "1. Define the MLP model using `torch.nn.Module`. The model should have:\n",
    "    - An input layer that matches the size of the TF-IDF features.\n",
    "    - Two hidden layers with ReLU activations.\n",
    "    - A single output layer with a sigmoid activation function.\n",
    "2. Print the model summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8179e1ea1d73d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:58:09.466766Z",
     "start_time": "2024-10-04T08:58:09.464703Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8773057adff1116a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:58:10.396727Z",
     "start_time": "2024-10-04T08:58:10.395179Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9266420be193ab7",
   "metadata": {},
   "source": [
    "### Exercise 6: Training the Model\n",
    "**Objective**: Train the MLP model on the training data.\n",
    "\n",
    "1. Convert the TF-IDF feature matrices and labels into PyTorch tensors (the label needs to be binarized).\n",
    "2. Define the loss function (`BCELoss` for binary classification) and the optimizer (`Adam`).\n",
    "3. Implement a training loop to train the model for a specified number of epochs (e.g., 50).\n",
    "4. Monitor the training and validation loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2698b469c554df8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:58:11.284843Z",
     "start_time": "2024-10-04T08:58:11.283287Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d316373ebf5c939a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:58:12.370745Z",
     "start_time": "2024-10-04T08:58:12.369210Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2527b1bffa065bf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:58:13.280467Z",
     "start_time": "2024-10-04T08:58:13.278957Z"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f0dec0e502aa8c",
   "metadata": {},
   "source": [
    "### Exercise 7: Model Evaluation\n",
    "**Objective**: Evaluate the performance of the trained model on the test data.\n",
    "\n",
    "1. Use the trained model to make predictions on the test set.\n",
    "2. Calculate the accuracy of the model on the test data.\n",
    "3. Print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05d21ad2985d22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:58:14.218747Z",
     "start_time": "2024-10-04T08:58:14.216975Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d47e836e3bccbf8",
   "metadata": {},
   "source": [
    "### **Exercise 8: Saving the Trained Model**\n",
    "\n",
    "1. Save the model's state_dict using `torch.save()`. \n",
    "\n",
    "2. Save the entire model, including its architecture and weights.\n",
    "\n",
    "3. Demonstrate how to load the saved model and use it for making predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad160c9e1373b3fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:58:15.209845Z",
     "start_time": "2024-10-04T08:58:15.208288Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146f5dd16930b67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:58:16.092378Z",
     "start_time": "2024-10-04T08:58:16.090902Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65cb2f47000ed68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:58:16.981545Z",
     "start_time": "2024-10-04T08:58:16.979910Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2d5294a91a521",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:58:17.864459Z",
     "start_time": "2024-10-04T08:58:17.862774Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1504f7c692bd8c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:58:18.736133Z",
     "start_time": "2024-10-04T08:58:18.734495Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b387d061cdd19d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T08:58:19.601840Z",
     "start_time": "2024-10-04T08:58:19.600228Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP From Scratch\n",
    "\n",
    "## Translation with a Sequence to Sequence Network and Attention\n",
    "===============================================================================\n",
    "\n",
    "In this project we will be teaching a neural network to translate from\n",
    "Portuguese to English.\n",
    "\n",
    "``` {.sh}\n",
    "[KEY: > input, = target]\n",
    "\n",
    "> Ele gosta de jogar futebol .\n",
    "= He likes playing football .\n",
    "\n",
    "> Eu sei contar até cem .\n",
    "= I know how to count to 100 .\n",
    "\n",
    "> Não fui capaz de encontrar o caminho .\n",
    "= I wasn't able to find my way out .\n",
    "```\n",
    "\n",
    "This is made possible by the simple but powerful idea of the [sequence\n",
    "to sequence network](https://arxiv.org/abs/1409.3215), in which two\n",
    "recurrent neural networks work together to transform one sequence to\n",
    "another. An encoder network condenses an input sequence into a vector,\n",
    "and a decoder network unfolds that vector into a new sequence.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/seq2seq.png)\n",
    "\n",
    "To improve upon this model we\\'ll use an [attention\n",
    "mechanism](https://arxiv.org/abs/1409.0473), which lets the decoder\n",
    "learn to focus over a specific range of the input sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading data files\n",
    "==================\n",
    "\n",
    "The data for this project is a set of many thousands of English to\n",
    "Portuguese translation pairs.\n",
    "\n",
    "Individual files with language pairs are available here: <https://www.manythings.org/anki/>\n",
    "\n",
    "The English to Portuguese pairs are available in this repository in the `por-eng` directory with the filename `por.txt`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be representing each word in a language as a one-hot\n",
    "vector. Compared to the dozens of characters that might exist in a\n",
    "language, there are many many more words, so the encoding vector is much\n",
    "larger. We will however cheat a bit and trim the data to only use a few\n",
    "thousand words per language.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/word-encoding.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We\\'ll need a unique index per word to use as the inputs and targets of\n",
    "the networks later. To keep track of all this we will use a helper class\n",
    "called `Lang` which has word → index (`word2index`) and index → word\n",
    "(`index2word`) dictionaries, as well as a count of each word\n",
    "`word2count` which will be used to replace rare words later.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercice 1: `Lang` class\n",
    "\n",
    "### 1. Implement the `Lang` class with the following methods:\n",
    "   - `__init__(self, name)`\n",
    "   - `add_sentence(self, sentence)`\n",
    "   - `addWord(self, word)`\n",
    "\n",
    "The `Lang` class should have the following attributes:\n",
    "    - `name`: the name of the language\n",
    "    - `word2index`: a dictionary mapping words to indexes, default = `{}`\n",
    "    - `word2count`: a dictionary mapping words to their count, default = `{}`\n",
    "    - `index2word`: a dictionary mapping indexes to words, default = `{0: \"SOS\", 1: \"EOS\"}`\n",
    "    - `n_words`: the number of words in the language, default = `2`\n",
    "    \n",
    "The `addSentence` method should split the sentence into words and call the `addWord` method for each word.\n",
    "\n",
    "The `add_word` method should add the word to the `word2index`, `word2count`, and `index2word` dictionaries if it is not already in the `word2index` dictionary. If the word is already in the `word2index` dictionary, increment the count of the word in the `word2count` dictionary.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:57:11.518776Z",
     "start_time": "2024-11-22T11:57:11.420862Z"
    }
   },
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count ={}\n",
    "        self.index2word = {0: 'SOS', 1: 'EQS'}\n",
    "        self.n_words = 2 \n",
    "        \n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "        "
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: Load Data and Text Processing\n",
    "\n",
    "### 1. Normalize the text \n",
    "\n",
    "Implement the following function:\n",
    "\n",
    "- `normalize_string(string)`: Lowercase, trim (strip), separate \".\", \"!\", \"?\", and words with spaces. Replace all non-letter characters with spaces (except for \".\", \"!\", \"?\"). Example: \"I am a student.\" -> \"i am a student .\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:57:11.558794Z",
     "start_time": "2024-11-22T11:57:11.521426Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "def normalize_string(string):\n",
    "    string = string.lower().strip()\n",
    "    string = re.sub(r\"([.!?])\", r\" \\1\", string)\n",
    "    string = re.sub(r\"[^\\w.!?]+\",r\" \" ,string)\n",
    "    \n",
    "    return string"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the data\n",
    "\n",
    "Implement the following function: \n",
    "\n",
    "- `load_data()`: Read the file and split into lines, split lines into pairs (eng-pt), and normalize each pair. Return the pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:57:12.785454Z",
     "start_time": "2024-11-22T11:57:11.563634Z"
    }
   },
   "source": [
    "def load_data():\n",
    "    with open('por-eng/por.txt', 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    pairs =[]\n",
    "    for line in lines:\n",
    "        l = line.split('\\t')\n",
    "        eng = l[0]\n",
    "        pt = l[1]\n",
    "        pair = (normalize_string(pt), normalize_string(eng))\n",
    "        pairs.append(pair)\n",
    "        \n",
    "    return pairs\n",
    "    \n",
    "load_data()[:5]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": "[('vai .', 'go .'),\n ('vá .', 'go .'),\n ('oi .', 'hi .'),\n ('corre !', 'run !'),\n ('corra !', 'run !')]"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filter the data\n",
    "\n",
    "Since there are a *lot* of example sentences and we want to train\n",
    "something quickly, we\\'ll trim the data set to only relatively short and\n",
    "simple sentences. Here the maximum length is 10 words (that includes\n",
    "ending punctuation).\n",
    "\n",
    "Implement the following function:\n",
    "\n",
    "- `filter_pair(p)`: Return `True` if the pair is shorter than the maximum length, i.e, if the length of the first sentence is less than 10 and the length of the second sentence is less than 10.\n",
    "\n",
    "- `filter_pairs(pairs)`: Return a list of pairs that satisfy the condition of the `filter_pair` function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:57:12.791699Z",
     "start_time": "2024-11-22T11:57:12.786531Z"
    }
   },
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "\n",
    "def filter_pair(p):\n",
    "    return len(p[0].split()) > MAX_LENGTH and len(p[1].split()) < MAX_LENGTH\n",
    "\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    return [pair for pair in pairs if filter_pair(pair)]"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. All the data processing steps\n",
    "\n",
    "Implement the following function:\n",
    "\n",
    "- `prepare_data(lang1, lang2)`: Read text file and split into lines, split lines into pairs, normalize text, filter by length, and make word lists from sentences in pairs. Return the input language (`Lang` object), output language (`Lang` object), and pairs.\n",
    "\n",
    "The full process for preparing the data is:\n",
    "\n",
    "-   Read text file and split into lines, split lines into pairs\n",
    "-   Normalize text, filter by length\n",
    "-   Make word lists from sentences in pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:57:13.964258Z",
     "start_time": "2024-11-22T11:57:12.789625Z"
    }
   },
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def prepare_data(lang1, lang2):\n",
    "    data = load_data()\n",
    "    pairs = filter_pairs(data)\n",
    "    lang1, lang2 = Lang(lang1), Lang(lang2)\n",
    "    for pair in pairs:\n",
    "        lang1.add_sentence(pair[0])\n",
    "        lang2.add_sentence(pair[1])\n",
    "    print(f\"{len(lang1.name)}: {lang1.n_words}\")\n",
    "    print(f\"{len(lang2.name)}: {lang2.n_words}\")\n",
    "    return lang1, lang2, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('pt', 'eng')\n",
    "print(random.choice(pairs))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2: 2724\n",
      "3: 2098\n",
      "('todos os funcionários tinham que decorar o código de acesso .', 'all employees had to memorize the access code .')\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Seq2Seq Model\n",
    "=================\n",
    "\n",
    "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
    "sequence and uses its own output as input for subsequent steps.\n",
    "\n",
    "A [Sequence to Sequence network](https://arxiv.org/abs/1409.3215), or\n",
    "seq2seq network, or [Encoder Decoder\n",
    "network](https://arxiv.org/pdf/1406.1078v3.pdf), is a model consisting\n",
    "of two RNNs called the encoder and decoder. The encoder reads an input\n",
    "sequence and outputs a single vector, and the decoder reads that vector\n",
    "to produce an output sequence.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/seq2seq.png)\n",
    "\n",
    "Unlike sequence prediction with a single RNN, where every input\n",
    "corresponds to an output, the seq2seq model frees us from sequence\n",
    "length and order, which makes it ideal for translation between two\n",
    "languages.\n",
    "\n",
    "Consider the sentence `Eu não sou o gato preto` →\n",
    "`I am not the black cat`. Most of the words in the input sentence have a\n",
    "direct translation in the output sentence, but are in slightly different\n",
    "orders, e.g. `gato preto` and `black cat`. Additionally, sometimes the length\n",
    "of the input sequence is different from the output sequence.\n",
    "\n",
    "It would be difficult to produce a correct translation directly from the sequence\n",
    "of input words.\n",
    "\n",
    "With a seq2seq model the encoder creates a single vector which, in the\n",
    "ideal case, encodes the \\\"meaning\\\" of the input sequence into a single\n",
    "vector --- a single point in some N dimensional space of sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Encoder\n",
    "===========\n",
    "\n",
    "The encoder of a seq2seq network is a RNN that outputs some value for\n",
    "every word from the input sentence. For every input word the encoder\n",
    "outputs a vector and a hidden state, and uses the hidden state for the\n",
    "next input word.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/encoder-network.png)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercice 3: EncoderRNN class\n",
    "\n",
    "### 1. Implement the `EncoderRNN` class with the following methods:\n",
    "   - `__init__(self, input_size, hidden_size, dropout_p=0.1)`: Initialize the encoder with the input size, hidden size, and dropout probability. The encoder should have an embedding layer, a GRU layer, and a dropout layer.\n",
    "   - `forward(self, input)`: Forward pass of the encoder. The input is passed through an embedding layer, followed by a GRU layer. The output and hidden state of the GRU layer are returned."
   ]
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T12:23:06.204232Z",
     "start_time": "2024-11-22T12:23:06.199899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        dropout = self.dropout(embedded)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Decoder\n",
    "===========\n",
    "\n",
    "The decoder is another RNN that takes the encoder output vector(s) and\n",
    "outputs a sequence of words to create the translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple Decoder\n",
    "==============\n",
    "\n",
    "In the simplest seq2seq decoder we use only last output of the encoder.\n",
    "This last output is sometimes called the *context vector* as it encodes\n",
    "context from the entire sequence. This context vector is used as the\n",
    "initial hidden state of the decoder.\n",
    "\n",
    "At every step of decoding, the decoder is given an input token and\n",
    "hidden state. The initial input token is the start-of-string `<SOS>`\n",
    "token, and the first hidden state is the context vector (the encoder\\'s\n",
    "last hidden state).\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/decoder-network.png)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercice 4: DecoderRNN class\n",
    "\n",
    "### 1. Implement the `DecoderRNN` class with the following methods:\n",
    "   - `__init__(self, hidden_size, output_size)`: Initialize the decoder with the hidden size and output size. The decoder should have an embedding layer, a GRU layer, and a linear layer.\n",
    "   - `forward(self, encoder_outputs, encoder_hidden, target_tensor=None)`: Forward pass of the decoder. The decoder takes the encoder outputs, encoder hidden state, and target tensor as input. The target tensor is used for teacher forcing. The decoder outputs are returned.\n",
    "   - `forward_step(self, input, hidden)`: Forward pass of the decoder for a single step. The input is passed through an embedding layer, followed by a GRU layer, and a linear layer. The output and hidden state of the GRU layer are returned."
   ]
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T12:49:50.846325Z",
     "start_time": "2024-11-22T12:49:50.841736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLu()\n",
    "        self.sotmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        _decoder_output = []\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_input, hidden = self.forward(encoder_outputs, encoder_hidden)\n",
    "            _decoder_output.append(decoder_output)\n",
    "            _decoder_input = _decoder_output[-1]\n",
    "            decoder_input = torch.cat((decoder_input, _decoder_input), 1)\n",
    "        decoder_output = torch.cat(_decoder_output, 1)\n",
    "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.relu(embedded)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        output = self.out(output)\n",
    "        output = self.sotmax(output)\n",
    "        \n",
    "        return output, hidden"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Decoder\n",
    "=================\n",
    "\n",
    "If only the context vector is passed between the encoder and decoder,\n",
    "that single vector carries the burden of encoding the entire sentence.\n",
    "\n",
    "Attention allows the decoder network to \\\"focus\\\" on a different part of\n",
    "the encoder\\'s outputs for every step of the decoder\\'s own outputs.\n",
    "First we calculate a set of *attention weights*. These will be\n",
    "multiplied by the encoder output vectors to create a weighted\n",
    "combination. The result (called `attn_applied` in the code) should\n",
    "contain information about that specific part of the input sequence, and\n",
    "thus help the decoder choose the right output words.\n",
    "\n",
    "![](img/1152PYf.png)\n",
    "\n",
    "Calculating the attention weights is done with another feed-forward\n",
    "layer `attn`, using the decoder\\'s input and hidden state as inputs.\n",
    "Because there are sentences of all sizes in the training data, to\n",
    "actually create and train this layer we have to choose a maximum\n",
    "sentence length (input length, for encoder outputs) that it can apply\n",
    "to. Sentences of the maximum length will use all the attention weights,\n",
    "while shorter sentences will only use the first few.\n",
    "\n",
    "![](https://pytorch.org/tutorials/_static/img/seq-seq-images/attention-decoder-network.png)\n",
    "\n",
    "Bahdanau attention, also known as additive attention, is a commonly used\n",
    "attention mechanism in sequence-to-sequence models, particularly in\n",
    "neural machine translation tasks. It was introduced by Bahdanau et al.\n",
    "in their paper titled [Neural Machine Translation by Jointly Learning to\n",
    "Align and Translate](https://arxiv.org/pdf/1409.0473.pdf). This\n",
    "attention mechanism employs a learned alignment model to compute\n",
    "attention scores between the encoder and decoder hidden states. It\n",
    "utilizes a feed-forward neural network to calculate alignment scores.\n",
    "\n",
    "However, there are alternative attention mechanisms available, such as\n",
    "Luong attention, which computes attention scores by taking the dot\n",
    "product between the decoder hidden state and the encoder hidden states.\n",
    "It does not involve the non-linear transformation used in Bahdanau\n",
    "attention.\n",
    "\n",
    "In this tutorial, we will be using Bahdanau attention. However, it would\n",
    "be a valuable exercise to explore modifying the attention mechanism to\n",
    "use Luong attention.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercice 5: BahdanauAttention and AttnDecoderRNN classes\n",
    "\n",
    "### 1. Implement the `BahdanauAttention` class with the following methods:\n",
    "   - `__init__(self, hidden_size)`: Initialize the attention mechanism with the hidden size. The attention mechanism should have three linear layers.\n",
    "   - `forward(self, query, keys)`: Forward pass of the attention mechanism. The query and keys are passed through the linear layers and the attention weights are calculated. The context vector and attention weights are returned.\n",
    "   \n",
    "### 2. Implement the `AttnDecoderRNN` class with the following methods:\n",
    "   - `__init__(self, hidden_size, output_size, dropout_p=0.1)`: Initialize the decoder with the hidden size, output size, and dropout probability. The decoder should have an embedding layer, an attention mechanism, a GRU layer, and a linear layer.\n",
    "  - `forward(self, encoder_outputs, encoder_hidden, target_tensor=None)`: Forward pass of the decoder. The decoder takes the encoder outputs, encoder hidden state, and target tensor as input. The target tensor is used for teacher forcing. The decoder outputs are returned.\n",
    "- `forward_step(self, input, hidden)`: Forward pass of the decoder for a single step. The input is passed through an embedding layer, followed by the attention mechanism, a GRU layer, and a linear layer. The output and hidden state of the GRU layer are returned.s"
   ]
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:57:13.970235Z",
     "start_time": "2024-11-22T11:57:13.969903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        # ...\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        # ...\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        # ...\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        # ...\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        # ...\n",
    "        return output, hidden, attn_weights"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "========\n",
    "\n",
    "Preparing Training Data\n",
    "-----------------------\n",
    "\n",
    "To train, for each pair we will need an input tensor (indexes of the\n",
    "words in the input sentence) and target tensor (indexes of the words in\n",
    "the target sentence). While creating these vectors we will append the\n",
    "EOS token to both sequences.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercice 6: Indexes and Tensors\n",
    "\n",
    "### 1. Implement the following functions:\n",
    "- `indexes_from_sentence(lang, sentence)`: Return a list of indexes from the sentence.\n",
    "- `tensorFromSentence(lang, sentence)`: Return a tensor from the sentence.\n",
    "- `tensors_from_pair(pair)`: Return a pair of tensors from the pair."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:57:13.975784Z",
     "start_time": "2024-11-22T11:57:13.970991Z"
    }
   },
   "source": [
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n",
    "\n",
    "\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    return # ...\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    # ...\n",
    "    return indexes\n",
    "\n",
    "def tensors_from_pair(pair):\n",
    "    # ...\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    # ...\n",
    "    return input_lang, output_lang, train_dataloader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Model\n",
    "==================\n",
    "\n",
    "To train we run the input sentence through the encoder, and keep track\n",
    "of every output and the latest hidden state. Then the decoder is given\n",
    "the `<SOS>` token as its first input, and the last hidden state of the\n",
    "encoder as its first hidden state.\n",
    "\n",
    "\\\"Teacher forcing\\\" is the concept of using the real target outputs as\n",
    "each next input, instead of using the decoder\\'s guess as the next\n",
    "input. Using teacher forcing causes it to converge faster but [when the\n",
    "trained network is exploited, it may exhibit\n",
    "instability](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf).\n",
    "\n",
    "You can observe outputs of teacher-forced networks that read with\n",
    "coherent grammar but wander far from the correct translation\n",
    "-intuitively it has learned to represent the output grammar and can\n",
    "\\\"pick up\\\" the meaning once the teacher tells it the first few words,\n",
    "but it has not properly learned how to create the sentence from the\n",
    "translation in the first place.\n",
    "\n",
    "Because of the freedom PyTorch\\'s autograd gives us, we can randomly\n",
    "choose to use teacher forcing or not with a simple if statement. Turn\n",
    "`teacher_forcing_ratio` up to use more of it.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercice 7: Training\n",
    "\n",
    "### 1. Implement the following functions:\n",
    "- `train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)`: Train the model for one epoch.\n"
   ]
  },
  {
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-22T11:57:13.972066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
    "    # ...\n",
    "    return loss"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 8: Helper functions\n",
    "\n",
    "### 1. Implement the following functions:\n",
    "- `as_minutes(s)`: Convert seconds to minutes.\n",
    "- `time_since(since, percent)`: Calculate the time since the start of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-22T11:57:13.973266Z"
    }
   },
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def as_minutes(s):\n",
    "    # ...\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    # ...\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercice 9: Training\n",
    "\n",
    "### 1. Implement the following function:\n",
    "- `train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100, plot_every=100)`: Train the model for a number of epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole training process looks like this:\n",
    "\n",
    "-   Start a timer\n",
    "-   Initialize optimizers and criterion\n",
    "-   Create set of training pairs\n",
    "-   Start empty losses array for plotting\n",
    "\n",
    "Then we call `train` many times and occasionally print the progress (%\n",
    "of examples, time so far, estimated time) and average loss.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-22T11:57:13.974302Z"
    }
   },
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100, plot_every=100):\n",
    "    # ..."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 10: Plotting\n",
    "\n",
    "### 1. Implement the following function:\n",
    "- `show_plot(points)`: Plot the points.\n",
    "\n",
    "================\n",
    "\n",
    "Plotting is done with matplotlib, using the array of loss values\n",
    "`plot_losses` saved while training.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-22T11:57:13.975238Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def show_plot(points):\n",
    "    # ..."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 11: Evaluation\n",
    "\n",
    "### 1. Implement the following functions:\n",
    "\n",
    "- `evaluate(encoder, decoder, sentence, input_lang, output_lang)`: Evaluate the model on a sentence.\n",
    "\n",
    "==========\n",
    "\n",
    "Evaluation is mostly the same as training, but there are no targets so\n",
    "we simply feed the decoder\\'s predictions back to itself for each step.\n",
    "Every time it predicts a word we add it to the output string, and if it\n",
    "predicts the EOS token we stop there. We also store the decoder\\'s\n",
    "attention outputs for display later.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-22T11:57:13.976793Z",
     "start_time": "2024-11-22T11:57:13.975881Z"
    }
   },
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    # ...\n",
    "    return decoded_words, decoder_attn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercice 12: Random Evaluation\n",
    "\n",
    "### 1. Implement the following function:\n",
    "- `evaluate_randomly(encoder, decoder, n=10)`: Evaluate the model on random sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate random sentences from the training set and print out the\n",
    "input, target, and output to make some subjective quality judgements:\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-22T11:57:13.976575Z"
    }
   },
   "source": [
    "def evaluate_randomly(encoder, decoder, n=10):\n",
    "    # ..."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluating\n",
    "=======================\n",
    "\n",
    "With all these helper functions in place (it looks like extra work, but\n",
    "it makes it easier to run multiple experiments) we can actually\n",
    "initialize a network and start training.\n",
    "\n",
    "Remember that the input sentences were heavily filtered. For this small\n",
    "dataset we can use relatively small networks of 256 hidden nodes and a\n",
    "single GRU layer. After about 40 minutes on a MacBook CPU we\\'ll get\n",
    "some reasonable results.\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exercice 13: Training\n",
    "\n",
    "### 1. Train the model for 80 epochs."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-22T11:57:13.977315Z"
    }
   },
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "# ..."
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "###  2. Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set dropout layers to `eval` mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-22T11:57:13.978019Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluate_randomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing Attention\n",
    "=====================\n",
    "\n",
    "A useful property of the attention mechanism is its highly interpretable\n",
    "outputs. Because it is used to weight specific encoder outputs of the\n",
    "input sequence, we can imagine looking where the network is focused most\n",
    "at each time step.\n",
    "\n",
    "You could simply run `plt.matshow(attentions)` to see attention output\n",
    "displayed as a matrix. For a better viewing experience we will do the\n",
    "extra work of adding axes and labels:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-11-22T11:57:13.978701Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_and_show_attention(input_sentence):\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    show_attention(input_sentence, output_words, attentions[0, :len(output_words), :])\n",
    "\n",
    "\n",
    "evaluate_and_show_attention('il n est pas aussi grand que son pere')\n",
    "\n",
    "evaluate_and_show_attention('je suis trop fatigue pour conduire')\n",
    "\n",
    "evaluate_and_show_attention('je suis desole si c est une question idiote')\n",
    "\n",
    "evaluate_and_show_attention('je suis reellement fiere de vous')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

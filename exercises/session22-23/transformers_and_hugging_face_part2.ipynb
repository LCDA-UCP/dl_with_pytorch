{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Using ü§ó Transformers"
   ],
   "id": "ea7cd3c6a9c8c6db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "### Overview of the ü§ó Transformers Library\n",
    "\n",
    "As you saw in [Part 1](transformers_and_hugging_face_part1.ipynb), Transformer models are typically very large, with millions to tens of billions of parameters. Training and deploying these models is a complex process. Additionally, new models are released almost daily, each with its own implementation, making it challenging to experiment with all of them.\n",
    "\n",
    "The ü§ó Transformers library was created to address these challenges by providing a unified API to load, train, and save any Transformer model. Its main features include:\n",
    "\n",
    "- **Ease of use**: Downloading, loading, and using a state-of-the-art NLP model for inference can be done in just two lines of code.\n",
    "- **Flexibility**: At their core, all models are simple `PyTorch nn.Module` or `TensorFlow tf.keras.Model` classes and can be treated like any other models in their respective machine learning frameworks.\n",
    "- **Simplicity**: The library minimizes abstractions. Each model‚Äôs forward pass is fully defined in a single file, following the ‚ÄúAll in one file‚Äù principle. This makes the code understandable and hackable.\n",
    "\n",
    "This last feature distinguishes ü§ó Transformers from other ML libraries. Instead of relying on shared modules across files, each model has its own dedicated layers. This approach not only makes the models more approachable and easier to understand but also allows experimentation with one model without impacting others.\n",
    "\n",
    "---\n",
    "\n",
    "### Highlights\n",
    "\n",
    "In this part, we will:\n",
    "\n",
    "1. Begin with an **end-to-end example** that combines a model and a tokenizer to replicate the `pipeline()` function introduced in Part 1.\n",
    "2. Explore the **model API**, including:\n",
    "   - The `model` and `configuration` classes.\n",
    "   - How to load a model and process numerical inputs to generate predictions.\n",
    "3. Dive into the **tokenizer API**, covering:\n",
    "   - Text-to-numerical conversion for neural networks.\n",
    "   - Numerical-to-text conversion for output interpretation.\n",
    "4. Learn to process **multiple sentences in a prepared batch** efficiently.\n",
    "5. Wrap up with a closer look at the high-level `tokenizer()` function.\n",
    "\n",
    "By the end of this part, you‚Äôll have a deeper understanding of how to leverage the ü§ó Transformers library for various NLP tasks.\n"
   ],
   "id": "7d584e3986472bc4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Behind the pipeline\n",
    "\n",
    "Let‚Äôs start with a complete example, taking a look at what happened behind the scenes when we executed the following code in Part 1:"
   ],
   "id": "47f4422cda54d1d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ")"
   ],
   "id": "8f4384e97d8add9f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "and obtained:"
   ],
   "id": "6af6489c76704738"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T16:32:57.693487Z",
     "start_time": "2024-11-30T16:32:57.685567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
    " {'label': 'NEGATIVE', 'score': 0.9994558095932007}]"
   ],
   "id": "c50745ee04367d20",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
       " {'label': 'NEGATIVE', 'score': 0.9994558095932007}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As we saw in Part 1, this pipeline groups together three steps: preprocessing, passing the inputs through the model, and postprocessing:\n",
    "\n",
    "<img src=\"img/pipeline.png\">\n",
    "\n",
    "Let‚Äôs break down the steps in the pipeline:"
   ],
   "id": "543135fc0753d15c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Preprocessing with a tokenizer\n",
    "\n",
    "Like other neural networks, Transformer models cannot process raw text directly. Therefore, the first step in any pipeline is to convert text inputs into numerical representations that the model can understand. This is done using a **tokenizer**, which handles the following tasks:\n",
    "\n",
    "1. **Splitting the input** into tokens, which could be words, subwords, or symbols (such as punctuation).\n",
    "2. **Mapping each token** to a unique integer.\n",
    "3. **Adding additional inputs** that the model may require (e.g., special tokens or padding).\n",
    "\n",
    "To ensure compatibility, this preprocessing must match exactly how the model was pretrained. This requires downloading the tokenizer configuration associated with the model. The `AutoTokenizer` class in ü§ó Transformers simplifies this process through its `from_pretrained()` method. \n",
    "\n",
    "Using the model‚Äôs checkpoint name, `from_pretrained()` fetches the necessary tokenizer information from the Model Hub and caches it locally, ensuring that the data is downloaded only on the first run.\n",
    "\n",
    "For example, the default checkpoint for the sentiment-analysis pipeline is **`distilbert-base-uncased-finetuned-sst-2-english`** (you can view its [model card](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)). To use it, run the following code:\n",
    "\n",
    "\n"
   ],
   "id": "c66e8220ab8ec885"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ],
   "id": "9b82c6cebd6c52e2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Once we have the tokenizer, we can pass our sentences directly to it, and it will return a dictionary ready to be fed into our model! However, Transformer models only accept **tensors** as input, so the final step is to convert the list of input IDs into tensors.\n",
    "\n",
    "ü§ó Transformers is framework-agnostic, meaning you don‚Äôt need to worry about the backend‚Äîwhether it‚Äôs PyTorch, TensorFlow, or Flax. However, since tensors are the expected input for these models, here‚Äôs a quick explanation: \n",
    "\n",
    "Tensors are similar to NumPy arrays. A tensor can be:\n",
    "- A **scalar** (0D),\n",
    "- A **vector** (1D),\n",
    "- A **matrix** (2D), or \n",
    "- Have higher dimensions.\n",
    "\n",
    "ML frameworks' tensors behave similarly to NumPy arrays and are just as simple to create. \n",
    "\n",
    "To specify the type of tensors to return (PyTorch, TensorFlow, or plain NumPy), we use the `return_tensors` argument. For example:\n"
   ],
   "id": "31cb11b93c8f304"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ],
   "id": "650cbe341783e5fd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The main things to remember here are that you can pass one sentence or a list of sentences, as well as specifying the type of tensors you want to get back (if no type is passed, you will get a list of lists as a result).\n",
    "\n",
    "Here‚Äôs what the results look like as PyTorch tensors:"
   ],
   "id": "ef1f79ecbe46a07c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "{\n",
    "    'input_ids': tensor([\n",
    "        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],\n",
    "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]\n",
    "    ]), \n",
    "    'attention_mask': tensor([\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    ])\n",
    "}\n",
    "```"
   ],
   "id": "b4ca84570e49b262"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The output from the tokenizer is a dictionary containing two keys: `input_ids` and `attention_mask`.\n",
    "\n",
    "- **`input_ids`**: This contains rows of integers, where each row represents a sentence. The integers are unique identifiers for the tokens in the corresponding sentence.\n",
    "- **`attention_mask`**: We'll explain the purpose of this key later.\n"
   ],
   "id": "ad5f11a9dbab2027"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Going through the model\n",
    "\n",
    "We can download our pretrained model the same way we did with our tokenizer. ü§ó Transformers provides an `AutoModel` class which also has a `from_pretrained()` method:"
   ],
   "id": "1b84773876dc9bc5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ],
   "id": "a99e0268a6d1f660"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "In this code snippet, we downloaded the same checkpoint we used in our pipeline earlier (it should already be cached) and instantiated a model with it.\n",
    "\n",
    "This architecture includes only the base Transformer module. Given some inputs, it outputs what we call **hidden states** (also referred to as **features**). For each model input, the Transformer model generates a high-dimensional vector that represents a contextual understanding of that input.\n",
    "\n",
    "Although these hidden states can be valuable on their own, they are typically passed as inputs to another part of the model called the **head**. In Part 1, we saw that different tasks could use the same Transformer architecture, but each task has its own specialized head associated with it.\n"
   ],
   "id": "89f6c20ae456028b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### A high-dimensional vector?\n",
    "\n",
    "The vector output by the Transformer module is typically large and has three dimensions:\n",
    "\n",
    "- **Batch size**: The number of sequences processed simultaneously (e.g., 2 in our example).\n",
    "- **Sequence length**: The length of the numerical representation of the sequence (e.g., 16 in our example).\n",
    "- **Hidden size**: The dimensionality of the vector for each model input. \n",
    "\n",
    "The term \"high dimensional\" refers to the **hidden size**, which can be quite large. For smaller models, this is often 768, while larger models may reach 3072 or more.\n",
    "\n",
    "We can observe this if we feed the preprocessed inputs to our model:\n"
   ],
   "id": "57e4ada492651c4e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "outputs = model(**inputs)\n",
    "print(outputs.last_hidden_state.shape)"
   ],
   "id": "d36f2fd84ee5d4cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "torch.Size([2, 16, 768])"
   ],
   "id": "61297a5af0529030"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Model heads: Making sense out of numbers\n",
    "\n",
    "The model heads take the high-dimensional vector of hidden states as input and project them onto a different dimension. They are usually composed of one or a few linear layers:\n",
    "\n",
    "<img src=\"img/head.svg\">"
   ],
   "id": "662a3aa0c36d913"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The output of the Transformer model is sent directly to the **model head** for further processing.\n",
    "\n",
    "#### Model Structure\n",
    "In the diagram, the model consists of:\n",
    "- **Embeddings layer**: Converts each input ID from the tokenized input into a vector representing the associated token.\n",
    "- **Subsequent layers**: Process these vectors using the attention mechanism to generate the final representation of the sentences.\n",
    "\n",
    "#### Model Architectures\n",
    "There are various architectures available in ü§ó Transformers, each tailored for specific tasks. Some examples include:\n",
    "\n",
    "- `AutoModel` (retrieve the hidden states)\n",
    "- `AutoModelForCausalLM`\n",
    "- `AutoModelForMaskedLM`\n",
    "- `AutoModelForMultipleChoice`\n",
    "- `AutoModelForQuestionAnswering`\n",
    "- `AutoModelForSequenceClassification`\n",
    "- `AutoModelForTokenClassification`\n",
    "- And more ü§ó\n",
    "\n",
    "#### Selecting the Right Model\n",
    "For our example, we require a model with a **sequence classification head** to classify sentences as positive or negative. Therefore, instead of using the `AutoModel` class, we‚Äôll use `AutoModelForSequenceClassification`:\n"
   ],
   "id": "b971bdd30f1da722"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ],
   "id": "5387650b295ed96e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now if we look at the shape of our outputs, the dimensionality will be much lower: the model head takes as input the high-dimensional vectors we saw before, and outputs vectors containing two values (one per label):"
   ],
   "id": "590a9df00a52e94e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(outputs.logits.shape)"
   ],
   "id": "49e593f479bc81d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "torch.Size([2, 2])"
   ],
   "id": "3937b46277b2a5c5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Since we have just two sentences and two labels, the result we get from our model is of shape 2 x 2."
   ],
   "id": "40fd2a9608fd7512"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Postprocessing the output\n",
    "\n",
    "The values we get as output from our model don‚Äôt necessarily make sense by themselves. Let‚Äôs take a look:"
   ],
   "id": "28620edcd9f2dbea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(outputs.logits)"
   ],
   "id": "464b70d32c305037"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "tensor([[-1.5607,  1.6123],\n",
    "        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)\n",
    "```"
   ],
   "id": "8a9ca4b6c3e1acaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Our model predicted the following logits:\n",
    "\n",
    "- For the first sentence: `[-1.5607, 1.6123]`\n",
    "- For the second sentence: `[4.1692, -3.3464]`\n",
    "\n",
    "These are raw, unnormalized scores outputted by the last layer of the model. To convert logits into probabilities, they must pass through a **SoftMax** layer. \n",
    "\n",
    "All ü§ó Transformers models output logits because the loss function used during training often combines the final activation function (e.g., SoftMax) with the actual loss function (e.g., cross entropy). This approach allows for better numerical stability and flexibility during training.\n"
   ],
   "id": "33893bc8f398aa17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ],
   "id": "2d627744e352c3ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "tensor([[4.0195e-02, 9.5980e-01],\n",
    "        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)\n",
    "```"
   ],
   "id": "2c77c4ec3186c3ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we can see that the model predicted:\n",
    "\n",
    "- `[0.0402, 0.9598]` for the first sentence\n",
    "- `[0.9995, 0.0005]` for the second sentence\n",
    "\n",
    "These are recognizable probability scores.\n",
    "\n",
    "To identify the labels corresponding to each position, we can inspect the `id2label` attribute of the model's configuration (we‚Äôll discuss this in more detail in the next section).\n"
   ],
   "id": "46a6dbdf26cead5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(model.config.id2label)"
   ],
   "id": "f2037e0199e02fee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "{0: 'NEGATIVE', 1: 'POSITIVE'}\n",
    "```"
   ],
   "id": "17d6c639304150ab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we can conclude that the model predicted the following:\n",
    "\n",
    "- **First sentence**: NEGATIVE: 0.0402, POSITIVE: 0.9598  \n",
    "- **Second sentence**: NEGATIVE: 0.9995, POSITIVE: 0.0005  \n",
    "\n",
    "We have successfully reproduced the three steps of the pipeline: \n",
    "\n",
    "1. **Preprocessing** with tokenizers  \n",
    "2. **Passing the inputs through the model**  \n",
    "3. **Postprocessing**  \n",
    "\n"
   ],
   "id": "7c5182395aa578b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Models\n",
    "\n",
    "In this section we‚Äôll take a closer look at creating and using a model. We‚Äôll use the `AutoModel` class, which is handy when you want to instantiate any model from a checkpoint.\n",
    "\n",
    "The `AutoModel` class and all of its relatives are actually simple wrappers over the wide variety of models available in the library. It‚Äôs a clever wrapper as it can automatically guess the appropriate model architecture for your checkpoint, and then instantiates a model with this architecture.\n",
    "\n",
    "However, if you know the type of model you want to use, you can use the class that defines its architecture directly. Let‚Äôs take a look at how this works with a BERT model.\n"
   ],
   "id": "6905e7d2aff2d3a6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Creating a Transformer model\n",
    "\n",
    "The first thing we‚Äôll need to do to initialize a `BERT` model is load a configuration object:"
   ],
   "id": "c4ede07c5f0bde87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "# Building the config\n",
    "config = BertConfig()\n",
    "\n",
    "# Building the model from the config\n",
    "model = BertModel(config)"
   ],
   "id": "db87e304989e4290"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The configuration contains many attributes that are used to build the model:"
   ],
   "id": "d668c5d45664334c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(config)"
   ],
   "id": "73b405af13bda7b9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "BertConfig {\n",
    "  [...]\n",
    "  \"hidden_size\": 768,\n",
    "  \"intermediate_size\": 3072,\n",
    "  \"max_position_embeddings\": 512,\n",
    "  \"num_attention_heads\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  [...]\n",
    "}\n",
    "```\n",
    "\n",
    "While you haven‚Äôt seen what all of these attributes do yet, you should recognize some of them: the `hidden_size` attribute defines the size of the `hidden_states` vector, and `num_hidden_layers` defines the number of layers the Transformer model has.\n"
   ],
   "id": "d37ccc10cb1bf334"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Different loading methods\n",
    "\n",
    "Creating a model from the default configuration initializes it with random values:"
   ],
   "id": "7fdbdde8b547955f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import BertConfig, BertModel\n",
    "\n",
    "config = BertConfig()\n",
    "model = BertModel(config)\n",
    "\n",
    "# Model is randomly initialized!"
   ],
   "id": "ce3d7a951d67dd90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The model can be used in this state, but it will output gibberish; it needs to be trained first. We could train the model from scratch on the task at hand, but as you saw in Part 1, this would require a long time and a lot of data, and it would have a non-negligible environmental impact. To avoid unnecessary and duplicated effort, it‚Äôs imperative to be able to share and reuse models that have already been trained.\n",
    "\n",
    "Loading a Transformer model that is already trained is simple ‚Äî we can do this using the `from_pretrained()` method:"
   ],
   "id": "d7f2caf8b8adebf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\")"
   ],
   "id": "d7f89ad6c1f5a765"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "As you saw earlier, we could replace `BertModel` with the equivalent `AutoModel` class. We‚Äôll do this from now on as this produces checkpoint-agnostic code; if your code works for one checkpoint, it should work seamlessly with another. This applies even if the architecture is different, as long as the checkpoint was trained for a similar task (for example, a sentiment analysis task).\n",
    "\n",
    "In the code sample above we didn‚Äôt use `BertConfig`, and instead loaded a pretrained model via the `bert-base-cased` identifier. This is a model checkpoint that was trained by the authors of BERT themselves; you can find more details about it in its model card.\n",
    "\n",
    "This model is now initialized with all the weights of the checkpoint. It can be used directly for inference on the tasks it was trained on, and it can also be fine-tuned on a new task. By training with pretrained weights rather than from scratch, we can quickly achieve good results.\n",
    "\n",
    "The weights have been downloaded and cached (so future calls to the `from_pretrained()` method won‚Äôt re-download them) in the cache folder, which defaults to `~/.cache/huggingface/transformers`. You can customize your cache folder by setting the `HF_HOME` environment variable.\n",
    "\n",
    "The identifier used to load the model can be the identifier of any model on the Model Hub, as long as it is compatible with the BERT architecture. The entire list of available BERT checkpoints can be found [here](https://huggingface.co/models?other=bert).\n"
   ],
   "id": "32918b9e6b94a447"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Saving methods\n",
    "\n",
    "Saving a model is as easy as loading one ‚Äî we use the `save_pretrained()` method, which is analogous to the `from_pretrained()` method:"
   ],
   "id": "b3f7470083087770"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.save_pretrained(\"directory_on_my_computer\")"
   ],
   "id": "a4b782bdea9219d5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This saves two files to your disk:\n",
    "\n",
    "- `pytorch_model.bin`: The weights of the model.\n",
    "- `config.json`: The configuration of the model."
   ],
   "id": "84e06c68e2dbb8ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "If you take a look at the `config.json` file, you‚Äôll recognize the attributes necessary to build the model architecture. This file also contains some metadata, such as where the checkpoint originated and what ü§ó Transformers version you were using when you last saved the checkpoint.\n",
    "\n",
    "The `pytorch_model.bin` file is known as the state dictionary; it contains all your model‚Äôs weights. The two files go hand in hand: the configuration is necessary to know your model‚Äôs architecture, while the model weights are your model‚Äôs parameters.\n"
   ],
   "id": "d4296290edbb867a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Using a Transformer model for inference\n",
    "\n",
    "Now that you know how to load and save a model, let‚Äôs try using it to make some predictions. Transformer models can only process numbers ‚Äî numbers that the tokenizer generates. But before we discuss tokenizers, let‚Äôs explore what inputs the model accepts.\n",
    "\n",
    "Tokenizers can take care of casting the inputs to the appropriate framework‚Äôs tensors, but to help you understand what‚Äôs going on, we‚Äôll take a quick look at what must be done before sending the inputs to the model.\n",
    "\n",
    "Let‚Äôs say we have a couple of sequences:"
   ],
   "id": "d1a75a0195ff1ef6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
   ],
   "id": "d8bbc7447d722bbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The tokenizer converts these to vocabulary indices which are typically called input IDs. Each sequence is now a list of numbers! The resulting output is:"
   ],
   "id": "ecd63e0b85f11ae1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T17:34:14.879234Z",
     "start_time": "2024-11-30T17:34:14.876186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encoded_sequences = [\n",
    "    [101, 7592, 999, 102],\n",
    "    [101, 4658, 1012, 102],\n",
    "    [101, 3835, 999, 102],\n",
    "]"
   ],
   "id": "fa36ba3ecda768bc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This is a list of encoded sequences: a list of lists. Tensors only accept rectangular shapes (think matrices)."
   ],
   "id": "21256f3569cce56f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "\n",
    "model_inputs = torch.tensor(encoded_sequences)"
   ],
   "id": "a4f0a3727962b37e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Using the tensors as inputs to the model\n",
    "\n",
    "Making use of the tensors with the model is extremely simple ‚Äî we just call the model with the inputs:"
   ],
   "id": "5f43643776cf61b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "output = model(model_inputs)"
   ],
   "id": "2d17254e6d9af7f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "While the model accepts a lot of different arguments, only the input IDs are necessary. We‚Äôll explain what the other arguments do and when they are required later, but first we need to take a closer look at the tokenizers that build the inputs that a Transformer model can understand.\n"
   ],
   "id": "c493df5c495cbebd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tokenizers\n",
    "\n",
    "Tokenizers are one of the core components of the NLP pipeline. They serve one purpose: to translate text into data that can be processed by the model. Models can only process numbers, so tokenizers need to convert our text inputs to numerical data. In this section, we‚Äôll explore exactly what happens in the tokenization pipeline.\n",
    "\n",
    "In NLP tasks, the data that is generally processed is raw text. Here‚Äôs an example of such text:\n",
    "\n",
    "``` text\n",
    "Jim Henson was a puppeteer\n",
    "```\n",
    "\n",
    "However, models can only process numbers, so we need to find a way to convert the raw text to numbers. That‚Äôs what the tokenizers do, and there are a lot of ways to go about this. The goal is to find the most meaningful representation ‚Äî that is, the one that makes the most sense to the model ‚Äî and, if possible, the smallest representation.\n",
    "\n",
    "Let‚Äôs take a look at some examples of tokenization algorithms, and try to answer some of the questions you may have about tokenization.\n",
    "\n"
   ],
   "id": "ab94a2cc5baa2629"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Word-based\n",
    "\n",
    "The first type of tokenizer that comes to mind is word-based. It‚Äôs generally very easy to set up and use with only a few rules, and it often yields decent results. For example, in the image below, the goal is to split the raw text into words and find a numerical representation for each of them:\n",
    "\n",
    "<img src=\"img/word_based_tokenization.svg\">\n",
    "\n",
    "There are different ways to split the text. For example, we could use whitespace to tokenize the text into words by applying Python‚Äôs `split()` function:"
   ],
   "id": "29c4e9afebce6b18"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenized_text = \"Jim Henson was a puppeteer\".split()\n",
    "print(tokenized_text)"
   ],
   "id": "5d72e22d0d104cf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "['Jim', 'Henson', 'was', 'a', 'puppeteer']\n",
    "```"
   ],
   "id": "f3b8953aac219ab2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There are also variations of word tokenizers that have extra rules for punctuation. With this kind of tokenizer, we can end up with some pretty large ‚Äúvocabularies,‚Äù where a vocabulary is defined by the total number of independent tokens that we have in our corpus.\n",
    "\n",
    "Each word gets assigned an ID, starting from 0 and going up to the size of the vocabulary. The model uses these IDs to identify each word.\n",
    "\n",
    "If we want to completely cover a language with a word-based tokenizer, we‚Äôll need to have an identifier for each word in the language, which will generate a huge amount of tokens. For example, there are over 500,000 words in the English language, so to build a map from each word to an input ID we‚Äôd need to keep track of that many IDs. Furthermore, words like ‚Äúdog‚Äù are represented differently from words like ‚Äúdogs‚Äù, and the model will initially have no way of knowing that ‚Äúdog‚Äù and ‚Äúdogs‚Äù are similar: it will identify the two words as unrelated. The same applies to other similar words, like ‚Äúrun‚Äù and ‚Äúrunning‚Äù, which the model will not see as being similar initially.\n",
    "\n",
    "Finally, we need a custom token to represent words that are not in our vocabulary. This is known as the ‚Äúunknown‚Äù token, often represented as ‚Äù[UNK]‚Äù or ‚Äù<unk>‚Äù. It‚Äôs generally a bad sign if you see that the tokenizer is producing a lot of these tokens, as it wasn‚Äôt able to retrieve a sensible representation of a word and you‚Äôre losing information along the way. The goal when crafting the vocabulary is to do it in such a way that the tokenizer tokenizes as few words as possible into the unknown token.\n",
    "\n",
    "One way to reduce the amount of unknown tokens is to go one level deeper, using a character-based tokenizer.\n"
   ],
   "id": "7137e17301544099"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Character-based\n",
    "\n",
    "Character-based tokenizers split the text into characters, rather than words. This has two primary benefits:\n",
    "\n",
    "1. **The vocabulary is much smaller.**\n",
    "2. **There are much fewer out-of-vocabulary (unknown) tokens, since every word can be built from characters.**\n",
    "\n",
    "But here too some questions arise concerning spaces and punctuation:\n",
    "\n",
    "<img src=\"img/character_based_tokenization.svg\">\n",
    "\n",
    "This approach isn‚Äôt perfect either. Since the representation is now based on characters rather than words, one could argue that, intuitively, it‚Äôs less meaningful: each character doesn‚Äôt mean a lot on its own, whereas that is the case with words. However, this again differs according to the language; in Chinese, for example, each character carries more information than a character in a Latin language.\n",
    "\n",
    "Another thing to consider is that we‚Äôll end up with a very large amount of tokens to be processed by our model: whereas a word would only be a single token with a word-based tokenizer, it can easily turn into 10 or more tokens when converted into characters.\n",
    "\n",
    "To get the best of both worlds, we can use a third technique that combines the two approaches: **subword tokenization**.\n",
    "\n"
   ],
   "id": "1dc1dea3a9d39c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Subword tokenization\n",
    "\n",
    "Subword tokenization algorithms rely on the principle that frequently used words should not be split into smaller subwords, but rare words should be decomposed into meaningful subwords.\n",
    "\n",
    "For instance, ‚Äúannoyingly‚Äù might be considered a rare word and could be decomposed into ‚Äúannoying‚Äù and ‚Äúly‚Äù. These are both likely to appear more frequently as standalone subwords, while at the same time the meaning of ‚Äúannoyingly‚Äù is kept by the composite meaning of ‚Äúannoying‚Äù and ‚Äúly‚Äù.\n",
    "\n",
    "Here is an example showing how a subword tokenization algorithm would tokenize the sequence ‚ÄúLet‚Äôs do tokenization!‚Äú:\n",
    "\n",
    "\n",
    "<img src=\"img/subword.svg\">\n",
    "\n",
    "These subwords end up providing a lot of semantic meaning: for instance, in the example above ‚Äútokenization‚Äù was split into ‚Äútoken‚Äù and ‚Äúization‚Äù, two tokens that have a semantic meaning while being space-efficient (only two tokens are needed to represent a long word). This allows us to have relatively good coverage with small vocabularies, and close to no unknown tokens.\n",
    "\n",
    "This approach is especially useful in agglutinative languages such as Turkish, where you can form (almost) arbitrarily long complex words by stringing together subwords.\n",
    "\n",
    "#### And more!\n",
    "Unsurprisingly, there are many more techniques out there. To name a few:\n",
    "- Byte-level BPE, as used in GPT-2\n",
    "- WordPiece, as used in BERT\n",
    "- SentencePiece or Unigram, as used in several multilingual models\n",
    "\n",
    "You should now have sufficient knowledge of how tokenizers work to get started with the API.\n"
   ],
   "id": "b620bd24e1cc1d6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Loading and saving\n",
    "\n",
    "Loading and saving tokenizers is as simple as it is with models. Actually, it‚Äôs based on the same two methods: `from_pretrained()` and `save_pretrained()`. These methods will load or save the algorithm used by the tokenizer (a bit like the architecture of the model) as well as its vocabulary (a bit like the weights of the model).\n",
    "\n",
    "Loading the BERT tokenizer trained with the same checkpoint as BERT is done the same way as loading the model, except we use the `BertTokenizer` class:\n"
   ],
   "id": "d05b88c2066b5909"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ],
   "id": "5490c1dc8ab1e3b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Similar to `AutoModel`, the `AutoTokenizer` class will grab the proper tokenizer class in the library based on the checkpoint name, and can be used directly with any checkpoint:"
   ],
   "id": "79b97ea0eb1c4875"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ],
   "id": "3334fafc3df1335f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We can now use the tokenizer as shown in the previous section:"
   ],
   "id": "c20e4b1be3b74205"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer(\"Using a Transformer network is simple\")"
   ],
   "id": "1c4380132b714950"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],\n",
    " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
    "```"
   ],
   "id": "dbddb670fa53ff64"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Saving a tokenizer is identical to saving a model:"
   ],
   "id": "32244871b0bc40c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenizer.save_pretrained(\"directory_on_my_computer\")"
   ],
   "id": "542f208c91773f01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`token_type_ids` and `attention_mask` are explained in the next section."
   ],
   "id": "a42ba8e7923db835"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Encoding\n",
    "\n",
    "Translating text to numbers is known as **encoding**. Encoding is done in a two-step process: the tokenization, followed by the conversion to input IDs.\n",
    "\n",
    "As we‚Äôve seen, the first step is to split the text into words (or parts of words, punctuation symbols, etc.), usually called tokens. There are multiple rules that can govern that process, which is why we need to instantiate the tokenizer using the name of the model, to make sure we use the same rules that were used when the model was pretrained.\n",
    "\n",
    "The second step is to convert those tokens into numbers, so we can build a tensor out of them and feed them to the model. To do this, the tokenizer has a **vocabulary**, which is the part we download when we instantiate it with the `from_pretrained()` method. Again, we need to use the same vocabulary used when the model was pretrained.\n",
    "\n",
    "To get a better understanding of the two steps, we‚Äôll explore them separately. Note that we will use some methods that perform parts of the tokenization pipeline separately to show you the intermediate results of those steps, but in practice, you should call the tokenizer directly on your inputs.\n"
   ],
   "id": "bed4e2a0fdb0266b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tokenization\n",
    "\n",
    "The tokenization process is done by the `tokenize()` method of the tokenizer:"
   ],
   "id": "eedd2c4ae8450fee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "sequence = \"Using a Transformer network is simple\"\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "print(tokens)"
   ],
   "id": "1749dea73b759598"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The output of this method is a list of strings, or tokens:"
   ],
   "id": "bb1c3feb2c31fdd5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']\n",
    "```"
   ],
   "id": "a6d9697b1dd631"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This tokenizer is a subword tokenizer: it splits the words until it obtains tokens that can be represented by its vocabulary. That‚Äôs the case here with transformer, which is split into two tokens: transform and ##er. The ## is a special character that indicates that the token is a continuation of the previous one."
   ],
   "id": "58b9f1fcd7cae469"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### From tokens to input IDs\n",
    "\n",
    "The conversion to input IDs is handled by the `convert_tokens_to_ids()` tokenizer method:"
   ],
   "id": "c003b48de51ba1eb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(ids)"
   ],
   "id": "7059b949e848979e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "[7993, 170, 11303, 1200, 2443, 1110, 3014]\n",
    "```"
   ],
   "id": "b63aa6ce024dc4f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "These outputs, once converted to the appropriate framework tensor, can then be used as inputs to a model as seen earlier."
   ],
   "id": "d6681d6c5733a049"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Decoding\n",
    "\n",
    "Decoding is going the other way around: from vocabulary indices, we want to get a string. This can be done with the `decode()` method as follows:"
   ],
   "id": "85b20036f83a3309"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])\n",
    "print(decoded_string)"
   ],
   "id": "9b91ffd5a04133a1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "'Using a Transformer network is simple'\n",
    "```"
   ],
   "id": "4c8d1d347b363e7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Note that the `decode` method not only converts the indices back to tokens, but also groups together the tokens that were part of the same words to produce a readable sentence. This behavior will be extremely useful when we use models that predict new text (either text generated from a prompt, or for sequence-to-sequence problems like translation or summarization).\n",
    "\n",
    "By now you should understand the atomic operations a tokenizer can handle: tokenization, conversion to IDs, and converting IDs back to a string.\n",
    "\n"
   ],
   "id": "6fa1f2cc91f78060"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Handling multiple sequences\n",
    "\n",
    "In the previous section, we explored the simplest of use cases: doing inference on a single sequence of a small length. However, some questions emerge already:\n",
    "\n",
    "- **How do we handle multiple sequences?**  \n",
    "- **How do we handle multiple sequences of different lengths?**  \n",
    "- **Are vocabulary indices the only inputs that allow a model to work well?**  \n",
    "- **Is there such a thing as too long of a sequence?**\n",
    "\n",
    "Let‚Äôs see what kinds of problems these questions pose, and how we can solve them using the ü§ó Transformers API.\n"
   ],
   "id": "b46d9f110f815404"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Models expect a batch of inputs\n",
    "\n",
    "In the previous exercise we saw how sequences get translated into lists of numbers. Let‚Äôs convert this list of numbers to a tensor and send it to the model:\n"
   ],
   "id": "990e3f4dc684190a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = torch.tensor(ids)\n",
    "# This line will fail.\n",
    "model(input_ids)"
   ],
   "id": "27391e3a8a2c7160"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)\n",
    "```"
   ],
   "id": "1f01acf410620656"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Oh no! Why did this fail? \n",
    "\n",
    "‚ÄúWe followed the steps from the pipeline in the previous section,‚Äù you might say. ‚ÄúWe tokenized the sequence, converted it to IDs, and then converted it to a tensor. What went wrong?‚Äù\n",
    "\n",
    "The problem is that we sent a single sequence to the model, whereas ü§ó Transformers models expect multiple sentences by default. Here we tried to do everything the tokenizer did behind the scenes when we applied it to a sequence. But if you look closely, you‚Äôll see that the tokenizer didn‚Äôt just convert the list of input IDs into a tensor ‚Äî it added a dimension on top of it:\n"
   ],
   "id": "47f49ac070087242"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"pt\")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ],
   "id": "51dfb59e047acb0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
    "          2607,  2026,  2878,  2166,  1012,   102]])\n",
    "```"
   ],
   "id": "ceeac0cb118fcc93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Let‚Äôs try again and add a new dimension:"
   ],
   "id": "62c80c95e23b252d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = torch.tensor([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ],
   "id": "4de95340ff6addbc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We print the input IDs as well as the resulting logits ‚Äî here‚Äôs the output:"
   ],
   "id": "3d624c6b81b337b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "Input IDs: [[ 1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,  2607, 2026,  2878,  2166,  1012]]\n",
    "Logits: [[-2.7276,  2.8789]]\n",
    "```"
   ],
   "id": "a485e497d12e5031"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Batching is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence.\n",
    "\n",
    "Batching allows the model to work when you feed it multiple sentences. Using multiple sequences is just as simple as building a batch with a single sequence. \n",
    "\n",
    "There‚Äôs a second issue, though. When you‚Äôre trying to batch together two (or more) sentences, they might be of different lengths. If you‚Äôve ever worked with tensors before, you know that they need to be of a rectangular shape, so you won‚Äôt be able to convert the list of input IDs into a tensor directly. \n",
    "\n",
    "To work around this problem, we usually **pad the inputs**.\n"
   ],
   "id": "94d538cb3ad32b16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Padding the Inputs\n",
    "\n",
    "The following list of lists cannot be converted to a tensor:"
   ],
   "id": "932a6ed976d82aa3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]\n",
    "```"
   ],
   "id": "6e7758975bccd626"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To work around this, we‚Äôll use **padding** to ensure our tensors have a rectangular shape. Padding ensures that all sentences in a batch have the same length by adding a special word called the **padding token** to the shorter sentences. \n",
    "\n",
    "For example, if you have 10 sentences with 10 words each and 1 sentence with 20 words, padding will make all sentences 20 words long by appending padding tokens to the shorter ones. \n",
    "\n",
    "In our example, the resulting tensor looks like this:\n"
   ],
   "id": "9316a1ff805a9755"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]"
   ],
   "id": "cf326b9173c10ba1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The padding token ID can be found in `tokenizer.pad_token_id`. Let‚Äôs use it to pad our sentences and then send them through the model, both individually and as a batch:\n",
    "\n",
    "Here‚Äôs how it looks in practice:\n"
   ],
   "id": "f2d1b1b7fd14fbed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(torch.tensor(sequence1_ids)).logits)\n",
    "print(model(torch.tensor(sequence2_ids)).logits)\n",
    "print(model(torch.tensor(batched_ids)).logits)"
   ],
   "id": "55d78a3c4998bbf9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)\n",
    "tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n",
    "tensor([[ 1.5694, -1.3895],\n",
    "        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)\n",
    "```"
   ],
   "id": "b9b68ff7c9184bcd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "There‚Äôs something wrong with the logits in our batched predictions: the second row should match the logits for the second sentence, but we‚Äôve got completely different values!\n",
    "\n",
    "This happens because the key feature of Transformer models is their attention layers, which contextualize each token by attending to all tokens in the sequence. Without additional instructions, these layers will also attend to the padding tokens, causing inconsistencies in the output.\n",
    "\n",
    "To ensure the same results when processing individual sentences of different lengths or a padded batch, we need to inform the model to ignore the padding tokens. This is achieved by using an **attention mask**.\n"
   ],
   "id": "3efdc5c19f194a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Attention Masks\n",
    "\n",
    "**Attention masks** are tensors with the same shape as the input IDs tensor, consisting of 0s and 1s:  \n",
    "- **1s** indicate tokens that should be attended to.  \n",
    "- **0s** indicate tokens that should be ignored by the model‚Äôs attention layers.  \n",
    "\n",
    "This ensures the model only considers meaningful tokens and ignores padding tokens during computations.\n",
    "\n",
    "Let‚Äôs revisit the previous example and include an attention mask:\n"
   ],
   "id": "512a92a8bed7334e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))\n",
    "print(outputs.logits)"
   ],
   "id": "49b23ec3060021d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "tensor([[ 1.5694, -1.3895],\n",
    "        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)\n",
    "```"
   ],
   "id": "1f4ae449c7595107"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now we get the same logits for the second sentence in the batch.\n",
    "\n",
    "Notice how the last value of the second sequence is a padding ID, which is a 0 value in the attention mask."
   ],
   "id": "a21e6b13fd214d24"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Longer sequences\n",
    "\n",
    "With Transformer models, there is a **limit to the sequence lengths** they can process. Most models support sequences of up to **512 or 1024 tokens**, and processing longer sequences will result in errors. To address this, you have two options:\n",
    "\n",
    "1. **Use a model designed for longer sequences:**  \n",
    "   Some models, like **Longformer** or **LED**, specialize in handling very long sequences. If your task involves extensive sequences, consider exploring these models.\n",
    "\n",
    "2. **Truncate your sequences:**  \n",
    "   Specify a `max_sequence_length` to ensure that sequences exceeding the limit are truncated.\n",
    "\n",
    "Here‚Äôs how you can apply truncation:\n"
   ],
   "id": "5be386233d0ab317"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sequence = sequence[:max_sequence_length]"
   ],
   "id": "aeca0bd6f4f8c7f0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Putting it all together\n",
    "\n",
    "In the previous sections, we manually handled various steps of the tokenization process, including tokenization, converting to input IDs, padding, truncation, and adding attention masks.\n",
    "\n",
    "However, as we already saw, the ü§ó Transformers API provides a high-level function that automates these tasks for us. By calling the tokenizer directly on the sentence, we can get inputs that are already prepared and ready to be passed through the model.\n",
    "\n",
    "Here's how it works:\n"
   ],
   "id": "c010eae8d05dd68a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ],
   "id": "2ee7e32a1c8539a0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Here, the `model_inputs` variable contains everything that‚Äôs necessary for a model to operate well. For DistilBERT, this includes the input IDs as well as the attention mask. Other models that require additional inputs will also output them through the tokenizer object.\n",
    "\n",
    "This method is very powerful. For example, it can tokenize a single sequence:\n"
   ],
   "id": "7817cc88db01a0e7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)"
   ],
   "id": "1690b7fc24ab7140"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It also handles multiple sequences at a time, with no change in the API:"
   ],
   "id": "f9533c130357506d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "model_inputs = tokenizer(sequences)"
   ],
   "id": "758623c65f1419f6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It can pad according to several objectives:"
   ],
   "id": "558ba90708d33166"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")\n",
    "\n",
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")\n",
    "\n",
    "# Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
   ],
   "id": "478b892f849b83a7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "It can also truncate sequences:"
   ],
   "id": "53ab4ebf525b1310"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Will truncate the sequences that are longer than the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)\n",
    "\n",
    "# Will truncate the sequences that are longer than the specified max length\n",
    "model_inputs = tokenizer(sequences, max_length=8, truncation=True)"
   ],
   "id": "77aeeb169a599225"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The tokenizer object can handle the conversion to specific framework tensors, which can then be directly sent to the model. For example, in the following code sample, we are prompting the tokenizer to return tensors from different frameworks ‚Äî \"pt\" returns PyTorch tensors, \"tf\" returns TensorFlow tensors, and \"np\" returns NumPy arrays:\n"
   ],
   "id": "711ff2465f21ff1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "# Returns PyTorch tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "# Returns TensorFlow tensors\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "# Returns NumPy arrays\n",
    "model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
   ],
   "id": "91c7d07be6e008d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Special tokens\n",
    "\n",
    "If we take a look at the input IDs returned by the tokenizer, we will see they are a tiny bit different from what we had earlier:"
   ],
   "id": "584f849ab2420b91"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs[\"input_ids\"])\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ],
   "id": "5a250bc6815c101b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
    "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n",
    "```"
   ],
   "id": "4827c6e0b09a2910"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "One token ID was added at the beginning, and one at the end. Let‚Äôs decode the two sequences of IDs above to see what this is about:"
   ],
   "id": "10294f646cf246c3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))\n",
    "print(tokenizer.decode(ids))"
   ],
   "id": "beac319fa47cdfdf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "``` python\n",
    "\"[CLS] i've been waiting for a huggingface course my whole life. [SEP]\"\n",
    "\"i've been waiting for a huggingface course my whole life.\"\n",
    "```"
   ],
   "id": "4274b89ba56facc0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The tokenizer ensures that the correct special tokens are added according to the specific model‚Äôs requirements. For example:\n",
    "\n",
    "- **[CLS]** (classification token) is typically added at the beginning for models like BERT, which is used for tasks like classification.\n",
    "- **[SEP]** (separator token) is added at the end to indicate the boundary between sentences or segments in tasks like question answering or sentence pair classification.\n",
    "\n",
    "Different models might require different special tokens, or even none at all, depending on the task and model architecture. The tokenizer handles this seamlessly, allowing you to focus on the data and task rather than worrying about specific model requirements.\n",
    "\n",
    "This automatic handling of special tokens helps ensure consistency and compatibility between your input data and the model during both training and inference.\n"
   ],
   "id": "e829fc322a77ac3b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Wrapping up: From tokenization to model\n",
    "\n",
    "Now that we‚Äôve seen all the individual steps the tokenizer object uses when applied on texts, let‚Äôs see one final time how it can handle multiple sequences (padding!), very long sequences (truncation!), and multiple types of tensors with its main API:"
   ],
   "id": "f7f47b1cd942556d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "tokens = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "output = model(**tokens)"
   ],
   "id": "3e9ea5fa1d899251"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "\n",
    "To recap, in this chapter you:\n",
    "\n",
    "- Learned the basic building blocks of a Transformer model.\n",
    "- Learned what makes up a tokenization pipeline.\n",
    "- Saw how to use a Transformer model in practice.\n",
    "- Learned how to leverage a tokenizer to convert text to tensors that are understandable by the model.\n",
    "- Set up a tokenizer and a model together to get from text to predictions.\n",
    "- Learned the limitations of input IDs, and learned about attention masks.\n",
    "- Played around with versatile and configurable tokenizer methods.\n",
    "\n",
    "From now on, you should be able to freely navigate the ü§ó Transformers docs: the vocabulary will sound familiar, and you‚Äôve already seen the methods that you‚Äôll use the majority of the time.\n"
   ],
   "id": "6f77e8bc3a3e7933"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Quiz\n",
    "\n",
    "### 1. What is the order of the language modeling pipeline?\n",
    "\n",
    "<ol type=\"a\">\n",
    "  <li>First, the model, which handles text and returns raw predictions. The tokenizer then makes sense of these predictions and converts them back to text when needed.</li>\n",
    "  <li> First, the tokenizer, which handles text and returns IDs. The model handles these IDs and outputs a prediction, which can be some text.</li>\n",
    "  <li>The tokenizer handles text and returns IDs. The model handles these IDs and outputs a prediction. The tokenizer can then be used once again to convert these predictions back to some text.</li>\n",
    "</ol>"
   ],
   "id": "b5c8bcf272d1c902"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. How many dimensions does the tensor output by the base Transformer model have, and what are they?\n",
    "\n",
    "<ol type=\"a\">\n",
    "  <li>2: The sequence length and the batch size</li>\n",
    "  <li>2: The sequence length and the hidden size</li>\n",
    "  <li>3: The sequence length, the batch size, and the hidden size</li>\n",
    "</ol>"
   ],
   "id": "793d213c9d77d56b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. What is a model head?\n",
    "\n",
    "<ol type=\"a\">\n",
    "  <li> A component of the base Transformer network that redirects tensors to their correct layers</li>\n",
    "  <li>Also known as the self-attention mechanism, it adapts the representation of a token according to the other tokens of the sequence</li>\n",
    "  <li>An additional component, usually made up of one or a few layers, to convert the transformer predictions to a task-specific output</li>\n",
    "</ol>"
   ],
   "id": "7d2ae3b26e331682"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. What is an AutoModel?\n",
    "\n",
    "<ol type=\"a\">\n",
    "  <li>A model that automatically trains on your data</li>\n",
    "  <li>An object that returns the correct architecture based on the checkpoint</li>\n",
    "  <li>A model that automatically detects the language used for its inputs to load the correct weights</li>\n",
    "</ol>"
   ],
   "id": "4f8aff73d639edfb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. What are the techniques to be aware of when batching sequences of different lengths together?\n",
    "\n",
    "<ol type=\"a\">\n",
    "  <li>Truncating</li>\n",
    "  <li>Returning tensors</li>\n",
    "  <li>Padding</li>\n",
    "  <li>Attention masking</li>\n",
    "</ol>"
   ],
   "id": "b7e43e4ba359b40a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. What is the point of applying a SoftMax function to the logits output by a sequence classification model?\n",
    "\n",
    "<ol type=\"a\">\n",
    "  <li>It softens the logits so that they're more reliable.</li>\n",
    "  <li>It applies a lower and upper bound so that they're understandable.</li>\n",
    "  <li>The total sum of the output is then 1, resulting in a possible probabilistic interpretation.</li>\n",
    "</ol>"
   ],
   "id": "8fdb536ce63c9c9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. What does the result variable contain in this code sample?\n",
    "\n",
    "``` python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "result = tokenizer.tokenize(\"Hello!\")\n",
    "```\n",
    "\n",
    "<ol type=\"a\">\n",
    "  <li>A list of strings, each string being a token</li>\n",
    "  <li>A list of IDs</li>\n",
    "  <li>A string containing all of the tokens</li>\n",
    "</ol>"
   ],
   "id": "7f8a6e160868d136"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 8. Is there something wrong with the following code?\n",
    "\n",
    "``` python\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "encoded = tokenizer(\"Hey!\", return_tensors=\"pt\")\n",
    "result = model(**encoded)\n",
    "```\n",
    "\n",
    "<ol type=\"a\">\n",
    "  <li>No, it seems correct.</li>\n",
    "  <li>The tokenizer and model should always be from the same checkpoint.</li>\n",
    "  <li>It's good practice to pad and truncate with the tokenizer as every input is a batch.</li>\n",
    "</ol>"
   ],
   "id": "2995b3e400b33492"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

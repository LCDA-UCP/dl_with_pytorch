{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RNNs Tutorial with PyTorch on the IMDB Dataset\n",
    "\n",
    "In this tutorial, we will build a Recurrent Neural Network (CNN) to classify movie reviews as positive or negative using the IMDB dataset.\n",
    " \n",
    "We'll go through the following steps:\n",
    "\n",
    "1. **Data Loading, Processing, and Augmentation**\n",
    "2. **Data Exploration**\n",
    "3. **Model Building**\n",
    "4. **Model Training**\n",
    "5. **Model Evaluation**\n",
    "\n"
   ],
   "id": "9cf2cd67a7fb4ec1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Data Loading, Processing, and Augmentation\n",
    "\n",
    "The IMDB dataset contains 50,000 movie reviews, split into 25,000 reviews for training and 25,000 reviews for testing. The dataset is preprocessed, and each review is encoded as a sequence of word indexes.\n",
    "\n",
    "### Exercise 1.1 - Data Loading and Processing\n",
    "\n",
    "1. Load the data from `'../../data/imdb_dataset.zip'` using pandas.\n",
    "2. Convert the `sentiment` column to a binary label (0 for negative and 1 for positive).\n",
    "3. Split the dataset into training and test sets (25,000 samples each).\n",
    "4. Preprocess the `review` column by converting it to lowercase and removing special characters, punctuation and stopwords. Additionally, use a stemming or lemmatization technique.\n",
    "5. Build a vocabulary of unique words in the training dataset.\n",
    "6. Tokenize the reviews using the vocabulary, i.e., replace each word with its index in the vocabulary. Note that the reviews should be padded to a fixed length.\n",
    "7. Create the data loaders for training and test datasets.\n"
   ],
   "id": "de854ac82e9a020b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T12:05:35.317402Z",
     "start_time": "2024-11-08T12:05:34.639028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd \n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('/Users/beatrizdias/dl_with_pytorch/data/imdb_dataset.zip', compression='zip')  \n",
    "df"
   ],
   "id": "d98248f80f8d704a",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  review sentiment\n0      One of the other reviewers has mentioned that ...  positive\n1      A wonderful little production. <br /><br />The...  positive\n2      I thought this was a wonderful way to spend ti...  positive\n3      Basically there's a family where a little boy ...  negative\n4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n...                                                  ...       ...\n49995  I thought this movie did a down right good job...  positive\n49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n49997  I am a Catholic taught in parochial elementary...  negative\n49998  I'm going to have to disagree with the previou...  negative\n49999  No one expects the Star Trek movies to be high...  negative\n\n[50000 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>49995</th>\n      <td>I thought this movie did a down right good job...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>49996</th>\n      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49997</th>\n      <td>I am a Catholic taught in parochial elementary...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49998</th>\n      <td>I'm going to have to disagree with the previou...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>49999</th>\n      <td>No one expects the Star Trek movies to be high...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>50000 rows Ã— 2 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T12:05:57.402492Z",
     "start_time": "2024-11-08T12:05:57.396078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "df.head()"
   ],
   "id": "4d35a16ffac61778",
   "outputs": [
    {
     "data": {
      "text/plain": "                                              review  sentiment\n0  One of the other reviewers has mentioned that ...          1\n1  A wonderful little production. <br /><br />The...          1\n2  I thought this was a wonderful way to spend ti...          1\n3  Basically there's a family where a little boy ...          0\n4  Petter Mattei's \"Love in the Time of Money\" is...          1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>review</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>One of the other reviewers has mentioned that ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I thought this was a wonderful way to spend ti...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Basically there's a family where a little boy ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:53:10.996547Z",
     "start_time": "2024-11-08T11:53:10.995054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train,test "
   ],
   "id": "b973103232079f75",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:53:10.999813Z",
     "start_time": "2024-11-08T11:53:10.997123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "2d5dc17df6189d98",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:53:11.001633Z",
     "start_time": "2024-11-08T11:53:10.999744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "ca686b162ef1e561",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:53:11.004569Z",
     "start_time": "2024-11-08T11:53:11.002141Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "570e293bcd709a1f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:53:11.007342Z",
     "start_time": "2024-11-08T11:53:11.004863Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "116fc3f49260ae1c",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:53:11.028842Z",
     "start_time": "2024-11-08T11:53:11.008700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "560106d252698ec9",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 1.2 - Data Augmentation\n",
    "\n",
    "1. Define a function to replace random words with synonyms from WordNet in the reviews.\n",
    "2. Augment the training dataset by replacing words in a few reviews with synonyms.\n",
    "\n",
    "Note: Never augment the test dataset. It's essential to evaluate the model on the original data."
   ],
   "id": "7a2ad79060d1313f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:53:27.033516Z",
     "start_time": "2024-11-08T11:53:11.014291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def augment_text(text, max_augment=3):\n",
    "    tokens = text.split()[:20]\n",
    "    augmented_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(max_augment):\n",
    "        if len(augmented_tokens) > 1:\n",
    "            rand_idx = random.randint(0, len(augmented_tokens) - 1)\n",
    "            word = augmented_tokens[rand_idx]\n",
    "            synonyms = wordnet.synsets(word)\n",
    "            if synonyms:\n",
    "                replacement = random.choice(synonyms).lemmas()[0].name()\n",
    "                augmented_tokens[rand_idx] = replacement\n",
    "    print(f\"Original: {' '.join(tokens)}\")\n",
    "    print(f\"Augmented: {' '.join(augmented_tokens)}\")\n",
    "    print('\\n\\n')\n",
    "    return ' '.join(augmented_tokens)\n",
    "\n",
    "# Apply augmentation to training set\n",
    "train_df['review'].iloc[:3].apply(lambda x: augment_text(x, max_augment=3))"
   ],
   "id": "cec1e57df2661d62",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcorpus\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m wordnet\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\n\u001B[1;32m      5\u001B[0m nltk\u001B[38;5;241m.\u001B[39mdownload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwordnet\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_with_pytorch/lib/python3.10/site-packages/nltk/__init__.py:146\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjsontags\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m    142\u001B[0m \u001B[38;5;66;03m###########################################################\u001B[39;00m\n\u001B[1;32m    143\u001B[0m \u001B[38;5;66;03m# PACKAGES\u001B[39;00m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;66;03m###########################################################\u001B[39;00m\n\u001B[0;32m--> 146\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchunk\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassify\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minference\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_with_pytorch/lib/python3.10/site-packages/nltk/chunk/__init__.py:155\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Natural Language Toolkit: Chunkers\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Copyright (C) 2001-2024 NLTK Project\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# For license information, see LICENSE.TXT\u001B[39;00m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;124;03mClasses and interfaces for identifying non-overlapping linguistic\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;124;03mgroups (such as base noun phrases) in unrestricted text.  This task is\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;124;03m     pattern is valid.\u001B[39;00m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 155\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchunk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ChunkParserI\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchunk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnamed_entity\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Maxent_NE_Chunker\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchunk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregexp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RegexpChunkParser, RegexpParser\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_with_pytorch/lib/python3.10/site-packages/nltk/chunk/api.py:13\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Natural Language Toolkit: Chunk parsing API\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Copyright (C) 2001-2024 NLTK Project\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m##  Chunk Parser Interface\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m##//////////////////////////////////////////////////////\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mchunk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ChunkScore\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minternals\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deprecated\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparse\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ParserI\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_with_pytorch/lib/python3.10/site-packages/nltk/chunk/util.py:12\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m accuracy \u001B[38;5;28;01mas\u001B[39;00m _accuracy\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtag\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmapping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m map_tag\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtag\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m str2tuple\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtree\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Tree\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_with_pytorch/lib/python3.10/site-packages/nltk/tag/__init__.py:72\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtag\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TaggerI\n\u001B[1;32m     71\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtag\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m str2tuple, tuple2str, untag\n\u001B[0;32m---> 72\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtag\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequential\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     73\u001B[0m     SequentialBackoffTagger,\n\u001B[1;32m     74\u001B[0m     ContextTagger,\n\u001B[1;32m     75\u001B[0m     DefaultTagger,\n\u001B[1;32m     76\u001B[0m     NgramTagger,\n\u001B[1;32m     77\u001B[0m     UnigramTagger,\n\u001B[1;32m     78\u001B[0m     BigramTagger,\n\u001B[1;32m     79\u001B[0m     TrigramTagger,\n\u001B[1;32m     80\u001B[0m     AffixTagger,\n\u001B[1;32m     81\u001B[0m     RegexpTagger,\n\u001B[1;32m     82\u001B[0m     ClassifierBasedTagger,\n\u001B[1;32m     83\u001B[0m     ClassifierBasedPOSTagger,\n\u001B[1;32m     84\u001B[0m )\n\u001B[1;32m     85\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtag\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbrill\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BrillTagger\n\u001B[1;32m     86\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtag\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbrill_trainer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BrillTaggerTrainer\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_with_pytorch/lib/python3.10/site-packages/nltk/tag/sequential.py:26\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m List, Optional, Tuple\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m jsontags\n\u001B[0;32m---> 26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassify\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m NaiveBayesClassifier\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprobability\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ConditionalFreqDist\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtag\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FeaturesetTaggerI, TaggerI\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_with_pytorch/lib/python3.10/site-packages/nltk/classify/__init__.py:97\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassify\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpositivenaivebayes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PositiveNaiveBayesClassifier\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassify\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrte_classify\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m RTEFeatureExtractor, rte_classifier, rte_features\n\u001B[0;32m---> 97\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassify\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mscikitlearn\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SklearnClassifier\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassify\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msenna\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Senna\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mclassify\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtextcat\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TextCat\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_with_pytorch/lib/python3.10/site-packages/nltk/classify/scikitlearn.py:38\u001B[0m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mprobability\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DictionaryProbDist\n\u001B[1;32m     37\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 38\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DictVectorizer\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LabelEncoder\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mImportError\u001B[39;00m:\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_with_pytorch/lib/python3.10/site-packages/sklearn/feature_extraction/__init__.py:3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"Feature extraction from raw data.\"\"\"\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m text\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_dict_vectorizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DictVectorizer\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_hash\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FeatureHasher\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_with_pytorch/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:31\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfixes\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _IS_32BIT\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mvalidation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FLOAT_DTYPES, check_array, check_is_fitted\n\u001B[0;32m---> 31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_hash\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m FeatureHasher\n\u001B[1;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_stop_words\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ENGLISH_STOP_WORDS\n\u001B[1;32m     34\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHashingVectorizer\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCountVectorizer\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstrip_tags\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     43\u001B[0m ]\n",
      "File \u001B[0;32m~/anaconda3/envs/dl_with_pytorch/lib/python3.10/site-packages/sklearn/feature_extraction/_hash.py:12\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseEstimator, TransformerMixin, _fit_context\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_param_validation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Interval, StrOptions\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_hashing_fast\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m transform \u001B[38;5;28;01mas\u001B[39;00m _hashing_transform\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_iteritems\u001B[39m(d):\n\u001B[1;32m     16\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Like d.iteritems, but accepts any collections.Mapping.\"\"\"\u001B[39;00m\n",
      "File \u001B[0;32m_hashing_fast.pyx:1\u001B[0m, in \u001B[0;36minit sklearn.feature_extraction._hashing_fast\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Data Exploration\n",
    "\n",
    "Before training our model, it's essential to explore the data. This helps us understand its distribution and visualize some examples.\n",
    "\n",
    "### Exercise 2 - Data Exploration\n",
    "1. Print the number of positive and negative reviews in the training and test datasets.\n",
    "2. Plot a histogram of the length of reviews in the training and test datasets.\n",
    "3. Print a few reviews and their corresponding sentiment labels."
   ],
   "id": "8c4d4b41cceb7666"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-08T11:53:27.033185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "3d3e17130dd15243",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:53:27.036344Z",
     "start_time": "2024-11-08T11:53:27.033792Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# length of reviews (histogram)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ..."
   ],
   "id": "757af2ea7a33129d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-08T11:53:27.035170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "ba41e3c8301a24c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T11:53:27.036825Z",
     "start_time": "2024-11-08T11:53:27.036503Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print reviews and sentiment labels\n",
    "# ..."
   ],
   "id": "c6b4941645567b17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Model Building\n",
    "\n",
    "Now, we'll define our RNN architecture. A typical RNN consists of an embedding layer, RNN layers, and a fully connected layers.\n",
    "\n",
    "### Exercise\n",
    "1. Define a RNN class inheriting from torch.nn.Module.\n",
    "2. Include one Embedding layer, two LSTM layers, and two Linear layers."
   ],
   "id": "ebf01dbc897fa0e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-08T11:53:27.037680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "86bc9381447392ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Model Training\n",
    "\n",
    "We'll now define the training loop to optimize our model using the binary cross-entropy loss function and the Adam optimizer.\n",
    "\n",
    "### Exercise\n",
    "1. Define a function to train the model for a specified number of epochs.\n",
    "2. Print the training loss after each epoch."
   ],
   "id": "ef72a0844d23519e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-08T11:53:27.038629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "60e9b2d2d4323664",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "After training, we need to evaluate our model on the test dataset to understand its performance.\n",
    "\n",
    "### Exercise\n",
    "1. Define a function to evaluate the model's accuracy on the test set.\n",
    "2. Print the accuracy."
   ],
   "id": "5ffdb68841ef1798"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-08T11:53:27.039418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "753013a29969972c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-08T11:53:27.040114Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "id": "f97e3a9ed9ac1b32",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

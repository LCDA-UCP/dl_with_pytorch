{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbc88e3cb2c788d8",
   "metadata": {},
   "source": [
    "# IMDB 50K Movie Reviews - Sentiment Classification with MLP\n",
    "\n",
    "## Overview\n",
    "\n",
    "### In the next exercises, you will work with the IMDB 50K Movie Reviews dataset to build a sentiment analysis model using a Multi-Layer Perceptron (MLP). You will practice essential steps in the data science pipeline such as data loading, preprocessing, feature generation, and training/testing a neural network model. This exercise should be completed using the `pandas`, `nltk`, `sklearn`, and `torch` libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1967f805bf9ca917",
   "metadata": {},
   "source": [
    "### Exercise 1: Data Loading and Exploration\n",
    "**Objective**: Load the IMDB dataset and explore its structure.\n",
    "\n",
    "1. **Load the dataset** using `pandas`. The dataset is in the `data/` folder and the file name is `imdb_dataset.zip`. **Hint: you can load zip files with pandas by passing `compression='zip'` tp `pd.read_csv`**\n",
    "2. **Explore the dataset** by checking for missing values and getting a summary of the data. \n",
    "    - Check the shape of the dataset.\n",
    "    - Get the distribution of the sentiment labels (positive/negative reviews).\n",
    "3. Print the first few reviews and their corresponding labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53cce52163c4447",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T13:05:39.877850Z",
     "start_time": "2024-10-03T13:05:38.293924Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import modin.pandas as pd # Optimized distributed version of Pandas\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('../../data/imdb_dataset.zip', compression='zip')  # Load as per your format\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e57bb2bb70f9f4ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:12:57.250598Z",
     "start_time": "2024-10-02T20:12:57.176593Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6388a96dd14227db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:12:57.475613Z",
     "start_time": "2024-10-02T20:12:57.306300Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: `df.groupby(categorical_by, sort=False)` implementation has mismatches with pandas:\n",
      "the groupby keys will be sorted anyway, although the 'sort=False' was passed. See the following issue for more details: https://github.com/modin-project/modin/issues/3571.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "negative    25000\n",
       "positive    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the distribution of labels\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50a1d1bcc857961",
   "metadata": {},
   "source": [
    "### Exercise 2: Splitting the Data\n",
    "**Objective**: Split the data into training, validation and test sets.\n",
    "\n",
    "1. Split the dataset into features (reviews) and labels (sentiment).\n",
    "2. Use `train_test_split` from `sklearn` to split the dataset into training, validation and test sets (use an 60/20/20 split).\n",
    "3. Print the sizes of the training, validation and test sets to ensure the splits were done correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd385320714117c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:12:58.834777Z",
     "start_time": "2024-10-02T20:12:58.213925Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data into features (X) and labels (y)\n",
    "X = df['review']\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7543576eeb9d21ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:12:59.147583Z",
     "start_time": "2024-10-02T20:12:59.041439Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split into training (60%), validation (20%), and test (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb278e805672ee97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:12:59.171551Z",
     "start_time": "2024-10-02T20:12:59.167222Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 30000\n",
      "Validation set size: 10000\n",
      "Test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565a8072f03328a",
   "metadata": {},
   "source": [
    "### Exercise 3: Text Preprocessing\n",
    "**Objective**: Preprocess the text data to prepare it for feature generation.\n",
    "\n",
    "1. Lowercase the text data. **Hint: python has a built-in string method for this**.\n",
    "2. Remove any URL from the reviews. **Hint: you can use regular expressions for this**.\n",
    "3. Remove non-word and non-whitespace characters (punctuation, special characters, etc.). **Hint: you can use regular expressions for this**.\n",
    "4. Remove digits. **Hint: you can use regular expressions for this**.\n",
    "5. Tokenize the reviews into individual words. **Hint: you can use the `nltk` library for this**.\n",
    "6. Remove stopwords. **Hint: you can use the `nltk` library for this**.\n",
    "7. Perform stemming or lemmatization. **Hint: you can use the `nltk` library for this**.\n",
    "8. Apply the preprocessing steps to both the training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "274001c7274aa06d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:13:01.467358Z",
     "start_time": "2024-10-02T20:13:00.961851Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/joao-\n",
      "[nltk_data]     correia/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/joao-\n",
      "[nltk_data]     correia/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20728d8e4d17ae5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:13:03.199321Z",
     "start_time": "2024-10-02T20:13:03.194888Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define a preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)  # Remove URLs\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove non-word/non-whitespace characters\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove digits\n",
    "    words = word_tokenize(text)  # Tokenize text\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stop words\n",
    "    words = [stemmer.stem(word) for word in words]  # Perform stemming\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "838d9aed37b5086b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:13:15.805449Z",
     "start_time": "2024-10-02T20:13:04.860096Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply preprocessing to the training, validation, and test sets\n",
    "X_train = X_train.apply(preprocess_text)\n",
    "X_val = X_val.apply(preprocess_text)\n",
    "X_test = X_test.apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "198fe3ef1a4716f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:13:19.292757Z",
     "start_time": "2024-10-02T20:13:19.283232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18306    borrow slightli modifi titl comment say usual ...\n",
       "49528    product account movi also got voic work entir ...\n",
       "44745    far one worst movi ever seen life watch practi...\n",
       "46827    obvious inspir seen sometim even gruesom blood...\n",
       "27531    movi almost gener defin import us born earli p...\n",
       "Name: review, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0dd5545db5d361",
   "metadata": {},
   "source": [
    "### Exercise 4: Feature Generation (TF-IDF)\n",
    "**Objective**: Convert the preprocessed text data into numerical features using TF-IDF.\n",
    "\n",
    "1. Use the `TfidfVectorizer` from `sklearn` to convert the reviews into numerical vectors.\n",
    "2. Limit the maximum number of features to 5,000 to reduce the dimensionality.\n",
    "3. Fit the vectorizer on the training set and transform both the training, validation and test sets.\n",
    "4. Print the shape of the transformed feature sets to confirm the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0ab295c4db2b89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:13:24.336749Z",
     "start_time": "2024-10-02T20:13:20.524053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30000, 5000), (10000, 5000), (10000, 5000))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the training data, transform the validation and test data\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train).toarray()\n",
    "X_val_tfidf = vectorizer.transform(X_val).toarray()\n",
    "X_test_tfidf = vectorizer.transform(X_test).toarray()\n",
    "\n",
    "X_train_tfidf.shape, X_val_tfidf.shape, X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61baaf1ab53478c0",
   "metadata": {},
   "source": [
    "### Exercise 5: Building the MLP Model (PyTorch)\n",
    "**Objective**: Build a simple Multi-Layer Perceptron (MLP) for binary classification.\n",
    "\n",
    "1. Define the MLP model using `torch.nn.Module`. The model should have:\n",
    "    - An input layer that matches the size of the TF-IDF features.\n",
    "    - Two hidden layers with ReLU activations.\n",
    "    - A single output layer with a sigmoid activation function.\n",
    "2. Print the model summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b8179e1ea1d73d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:13:25.574403Z",
     "start_time": "2024-10-02T20:13:24.404599Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 1)  # Binary classification\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8773057adff1116a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:13:26.883839Z",
     "start_time": "2024-10-02T20:13:26.745331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "MLP(\n",
      "  (fc1): Linear(in_features=5000, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the model\n",
    "input_dim = X_train_tfidf.shape[1]\n",
    "model = MLP(input_dim).to(device)\n",
    "\n",
    "# Print the model summary\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9266420be193ab7",
   "metadata": {},
   "source": [
    "### Exercise 6: Training the Model\n",
    "**Objective**: Train the MLP model on the training data.\n",
    "\n",
    "1. Convert the TF-IDF feature matrices and labels into PyTorch tensors (the label needs to be binarized).\n",
    "2. Define the loss function (`BCELoss` for binary classification) and the optimizer (`Adam`).\n",
    "3. Implement a training loop to train the model for a specified number of epochs (e.g., 50).\n",
    "4. Monitor the training and validation loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2698b469c554df8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:13:28.988483Z",
     "start_time": "2024-10-02T20:13:28.055673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert data to tensors\n",
    "X_train_tensor = torch.tensor(X_train_tfidf, dtype=torch.float32).to(device)\n",
    "y_train = y_train.map({'positive': 1, 'negative': 0})\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_val_tensor = torch.tensor(X_val_tfidf, dtype=torch.float32).to(device)\n",
    "y_val = y_val.map({'positive': 1, 'negative': 0})\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32).view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d316373ebf5c939a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:13:31.965268Z",
     "start_time": "2024-10-02T20:13:30.913561Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2527b1bffa065bf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:13:36.811433Z",
     "start_time": "2024-10-02T20:13:33.551814Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Training Loss: 0.6936, Validation Loss: 0.6928, Validation Accuracy: 49.90%\n",
      "Epoch [2/100], Training Loss: 0.6926, Validation Loss: 0.6916, Validation Accuracy: 49.90%\n",
      "Epoch [3/100], Training Loss: 0.6915, Validation Loss: 0.6901, Validation Accuracy: 49.90%\n",
      "Epoch [4/100], Training Loss: 0.6899, Validation Loss: 0.6882, Validation Accuracy: 50.93%\n",
      "Epoch [5/100], Training Loss: 0.6879, Validation Loss: 0.6860, Validation Accuracy: 62.51%\n",
      "Epoch [6/100], Training Loss: 0.6855, Validation Loss: 0.6835, Validation Accuracy: 73.61%\n",
      "Epoch [7/100], Training Loss: 0.6829, Validation Loss: 0.6807, Validation Accuracy: 79.47%\n",
      "Epoch [8/100], Training Loss: 0.6799, Validation Loss: 0.6776, Validation Accuracy: 81.69%\n",
      "Epoch [9/100], Training Loss: 0.6765, Validation Loss: 0.6741, Validation Accuracy: 82.44%\n",
      "Epoch [10/100], Training Loss: 0.6728, Validation Loss: 0.6703, Validation Accuracy: 82.64%\n",
      "Epoch [11/100], Training Loss: 0.6687, Validation Loss: 0.6661, Validation Accuracy: 82.47%\n",
      "Epoch [12/100], Training Loss: 0.6642, Validation Loss: 0.6615, Validation Accuracy: 82.50%\n",
      "Epoch [13/100], Training Loss: 0.6593, Validation Loss: 0.6564, Validation Accuracy: 82.39%\n",
      "Epoch [14/100], Training Loss: 0.6539, Validation Loss: 0.6510, Validation Accuracy: 82.49%\n",
      "Epoch [15/100], Training Loss: 0.6481, Validation Loss: 0.6452, Validation Accuracy: 82.60%\n",
      "Epoch [16/100], Training Loss: 0.6419, Validation Loss: 0.6391, Validation Accuracy: 82.60%\n",
      "Epoch [17/100], Training Loss: 0.6354, Validation Loss: 0.6327, Validation Accuracy: 82.69%\n",
      "Epoch [18/100], Training Loss: 0.6285, Validation Loss: 0.6259, Validation Accuracy: 82.72%\n",
      "Epoch [19/100], Training Loss: 0.6211, Validation Loss: 0.6188, Validation Accuracy: 82.64%\n",
      "Epoch [20/100], Training Loss: 0.6134, Validation Loss: 0.6112, Validation Accuracy: 82.71%\n",
      "Epoch [21/100], Training Loss: 0.6053, Validation Loss: 0.6034, Validation Accuracy: 82.75%\n",
      "Epoch [22/100], Training Loss: 0.5968, Validation Loss: 0.5952, Validation Accuracy: 82.83%\n",
      "Epoch [23/100], Training Loss: 0.5879, Validation Loss: 0.5866, Validation Accuracy: 82.89%\n",
      "Epoch [24/100], Training Loss: 0.5786, Validation Loss: 0.5776, Validation Accuracy: 83.04%\n",
      "Epoch [25/100], Training Loss: 0.5688, Validation Loss: 0.5683, Validation Accuracy: 83.08%\n",
      "Epoch [26/100], Training Loss: 0.5587, Validation Loss: 0.5586, Validation Accuracy: 83.21%\n",
      "Epoch [27/100], Training Loss: 0.5481, Validation Loss: 0.5486, Validation Accuracy: 83.47%\n",
      "Epoch [28/100], Training Loss: 0.5372, Validation Loss: 0.5383, Validation Accuracy: 83.74%\n",
      "Epoch [29/100], Training Loss: 0.5259, Validation Loss: 0.5278, Validation Accuracy: 83.89%\n",
      "Epoch [30/100], Training Loss: 0.5144, Validation Loss: 0.5173, Validation Accuracy: 84.09%\n",
      "Epoch [31/100], Training Loss: 0.5028, Validation Loss: 0.5066, Validation Accuracy: 84.27%\n",
      "Epoch [32/100], Training Loss: 0.4910, Validation Loss: 0.4958, Validation Accuracy: 84.50%\n",
      "Epoch [33/100], Training Loss: 0.4790, Validation Loss: 0.4850, Validation Accuracy: 84.71%\n",
      "Epoch [34/100], Training Loss: 0.4670, Validation Loss: 0.4742, Validation Accuracy: 84.87%\n",
      "Epoch [35/100], Training Loss: 0.4549, Validation Loss: 0.4635, Validation Accuracy: 84.96%\n",
      "Epoch [36/100], Training Loss: 0.4429, Validation Loss: 0.4530, Validation Accuracy: 85.15%\n",
      "Epoch [37/100], Training Loss: 0.4310, Validation Loss: 0.4426, Validation Accuracy: 85.26%\n",
      "Epoch [38/100], Training Loss: 0.4192, Validation Loss: 0.4325, Validation Accuracy: 85.42%\n",
      "Epoch [39/100], Training Loss: 0.4075, Validation Loss: 0.4226, Validation Accuracy: 85.60%\n",
      "Epoch [40/100], Training Loss: 0.3961, Validation Loss: 0.4131, Validation Accuracy: 85.69%\n",
      "Epoch [41/100], Training Loss: 0.3850, Validation Loss: 0.4040, Validation Accuracy: 85.89%\n",
      "Epoch [42/100], Training Loss: 0.3742, Validation Loss: 0.3953, Validation Accuracy: 86.05%\n",
      "Epoch [43/100], Training Loss: 0.3638, Validation Loss: 0.3870, Validation Accuracy: 86.09%\n",
      "Epoch [44/100], Training Loss: 0.3537, Validation Loss: 0.3792, Validation Accuracy: 86.16%\n",
      "Epoch [45/100], Training Loss: 0.3441, Validation Loss: 0.3719, Validation Accuracy: 86.27%\n",
      "Epoch [46/100], Training Loss: 0.3348, Validation Loss: 0.3651, Validation Accuracy: 86.34%\n",
      "Epoch [47/100], Training Loss: 0.3260, Validation Loss: 0.3587, Validation Accuracy: 86.48%\n",
      "Epoch [48/100], Training Loss: 0.3177, Validation Loss: 0.3529, Validation Accuracy: 86.49%\n",
      "Epoch [49/100], Training Loss: 0.3097, Validation Loss: 0.3476, Validation Accuracy: 86.57%\n",
      "Epoch [50/100], Training Loss: 0.3022, Validation Loss: 0.3428, Validation Accuracy: 86.47%\n",
      "Epoch [51/100], Training Loss: 0.2951, Validation Loss: 0.3384, Validation Accuracy: 86.35%\n",
      "Epoch [52/100], Training Loss: 0.2885, Validation Loss: 0.3345, Validation Accuracy: 86.42%\n",
      "Epoch [53/100], Training Loss: 0.2822, Validation Loss: 0.3310, Validation Accuracy: 86.51%\n",
      "Epoch [54/100], Training Loss: 0.2763, Validation Loss: 0.3280, Validation Accuracy: 86.55%\n",
      "Epoch [55/100], Training Loss: 0.2707, Validation Loss: 0.3253, Validation Accuracy: 86.66%\n",
      "Epoch [56/100], Training Loss: 0.2655, Validation Loss: 0.3231, Validation Accuracy: 86.68%\n",
      "Epoch [57/100], Training Loss: 0.2605, Validation Loss: 0.3211, Validation Accuracy: 86.68%\n",
      "Epoch [58/100], Training Loss: 0.2559, Validation Loss: 0.3195, Validation Accuracy: 86.73%\n",
      "Epoch [59/100], Training Loss: 0.2515, Validation Loss: 0.3182, Validation Accuracy: 86.68%\n",
      "Epoch [60/100], Training Loss: 0.2474, Validation Loss: 0.3171, Validation Accuracy: 86.71%\n",
      "Epoch [61/100], Training Loss: 0.2434, Validation Loss: 0.3163, Validation Accuracy: 86.71%\n",
      "Epoch [62/100], Training Loss: 0.2397, Validation Loss: 0.3158, Validation Accuracy: 86.72%\n",
      "Epoch [63/100], Training Loss: 0.2362, Validation Loss: 0.3154, Validation Accuracy: 86.68%\n",
      "Epoch [64/100], Training Loss: 0.2329, Validation Loss: 0.3152, Validation Accuracy: 86.76%\n",
      "Epoch [65/100], Training Loss: 0.2297, Validation Loss: 0.3152, Validation Accuracy: 86.85%\n",
      "Epoch [66/100], Training Loss: 0.2267, Validation Loss: 0.3154, Validation Accuracy: 86.86%\n",
      "Epoch [67/100], Training Loss: 0.2238, Validation Loss: 0.3156, Validation Accuracy: 86.87%\n",
      "Epoch [68/100], Training Loss: 0.2210, Validation Loss: 0.3161, Validation Accuracy: 86.87%\n",
      "Epoch [69/100], Training Loss: 0.2184, Validation Loss: 0.3166, Validation Accuracy: 86.84%\n",
      "Epoch [70/100], Training Loss: 0.2158, Validation Loss: 0.3172, Validation Accuracy: 86.80%\n",
      "Epoch [71/100], Training Loss: 0.2134, Validation Loss: 0.3179, Validation Accuracy: 86.83%\n",
      "Epoch [72/100], Training Loss: 0.2111, Validation Loss: 0.3187, Validation Accuracy: 86.88%\n",
      "Epoch [73/100], Training Loss: 0.2089, Validation Loss: 0.3195, Validation Accuracy: 86.86%\n",
      "Epoch [74/100], Training Loss: 0.2067, Validation Loss: 0.3204, Validation Accuracy: 86.74%\n",
      "Epoch [75/100], Training Loss: 0.2046, Validation Loss: 0.3214, Validation Accuracy: 86.74%\n",
      "Epoch [76/100], Training Loss: 0.2027, Validation Loss: 0.3224, Validation Accuracy: 86.72%\n",
      "Epoch [77/100], Training Loss: 0.2007, Validation Loss: 0.3234, Validation Accuracy: 86.69%\n",
      "Epoch [78/100], Training Loss: 0.1989, Validation Loss: 0.3245, Validation Accuracy: 86.64%\n",
      "Epoch [79/100], Training Loss: 0.1971, Validation Loss: 0.3256, Validation Accuracy: 86.66%\n",
      "Epoch [80/100], Training Loss: 0.1954, Validation Loss: 0.3267, Validation Accuracy: 86.74%\n",
      "Epoch [81/100], Training Loss: 0.1937, Validation Loss: 0.3279, Validation Accuracy: 86.72%\n",
      "Epoch [82/100], Training Loss: 0.1921, Validation Loss: 0.3291, Validation Accuracy: 86.71%\n",
      "Epoch [83/100], Training Loss: 0.1906, Validation Loss: 0.3302, Validation Accuracy: 86.75%\n",
      "Epoch [84/100], Training Loss: 0.1891, Validation Loss: 0.3314, Validation Accuracy: 86.75%\n",
      "Epoch [85/100], Training Loss: 0.1877, Validation Loss: 0.3326, Validation Accuracy: 86.71%\n",
      "Epoch [86/100], Training Loss: 0.1863, Validation Loss: 0.3338, Validation Accuracy: 86.63%\n",
      "Epoch [87/100], Training Loss: 0.1849, Validation Loss: 0.3350, Validation Accuracy: 86.57%\n",
      "Epoch [88/100], Training Loss: 0.1836, Validation Loss: 0.3362, Validation Accuracy: 86.50%\n",
      "Epoch [89/100], Training Loss: 0.1823, Validation Loss: 0.3375, Validation Accuracy: 86.54%\n",
      "Epoch [90/100], Training Loss: 0.1811, Validation Loss: 0.3387, Validation Accuracy: 86.50%\n",
      "Epoch [91/100], Training Loss: 0.1799, Validation Loss: 0.3399, Validation Accuracy: 86.46%\n",
      "Epoch [92/100], Training Loss: 0.1787, Validation Loss: 0.3411, Validation Accuracy: 86.40%\n",
      "Epoch [93/100], Training Loss: 0.1775, Validation Loss: 0.3424, Validation Accuracy: 86.36%\n",
      "Epoch [94/100], Training Loss: 0.1764, Validation Loss: 0.3436, Validation Accuracy: 86.37%\n",
      "Epoch [95/100], Training Loss: 0.1753, Validation Loss: 0.3448, Validation Accuracy: 86.38%\n",
      "Epoch [96/100], Training Loss: 0.1743, Validation Loss: 0.3461, Validation Accuracy: 86.33%\n",
      "Epoch [97/100], Training Loss: 0.1733, Validation Loss: 0.3473, Validation Accuracy: 86.35%\n",
      "Epoch [98/100], Training Loss: 0.1723, Validation Loss: 0.3486, Validation Accuracy: 86.37%\n",
      "Epoch [99/100], Training Loss: 0.1713, Validation Loss: 0.3499, Validation Accuracy: 86.41%\n",
      "Epoch [100/100], Training Loss: 0.1703, Validation Loss: 0.3511, Validation Accuracy: 86.41%\n"
     ]
    }
   ],
   "source": [
    "# Training loop with validation monitoring\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    \n",
    "    # Forward pass on training data\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation step (model evaluation on validation set)\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "        val_predictions = (val_outputs > 0.5).float()\n",
    "        val_accuracy = (val_predictions.eq(y_val_tensor).sum() / y_val_tensor.shape[0]).item()\n",
    "    \n",
    "    # Print training and validation stats\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Training Loss: {loss.item():.4f}, '\n",
    "          f'Validation Loss: {val_loss.item():.4f}, '\n",
    "          f'Validation Accuracy: {val_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f0dec0e502aa8c",
   "metadata": {},
   "source": [
    "### Exercise 7: Model Evaluation\n",
    "**Objective**: Evaluate the performance of the trained model on the test data.\n",
    "\n",
    "1. Use the trained model to make predictions on the test set.\n",
    "2. Calculate the accuracy of the model on the test data.\n",
    "3. Print the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d05d21ad2985d22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:13:38.654291Z",
     "start_time": "2024-10-02T20:13:38.404446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 86.28%\n"
     ]
    }
   ],
   "source": [
    "# Convert test data to tensors\n",
    "X_test_tensor = torch.tensor(X_test_tfidf, dtype=torch.float32).to(device)\n",
    "y_test = y_test.map({'positive': 1, 'negative': 0})\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on the test set\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test_tensor)\n",
    "    test_predictions = (test_outputs > 0.5).float()  # Convert probabilities to binary predictions\n",
    "    test_accuracy = (test_predictions.eq(y_test_tensor).sum() / y_test_tensor.shape[0]).item()\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d47e836e3bccbf8",
   "metadata": {},
   "source": [
    "### **Exercise 8: Saving the Trained Model**\n",
    "\n",
    "1. Save the model's state_dict using `torch.save()`. \n",
    "\n",
    "2. Save the entire model, including its architecture and weights.\n",
    "\n",
    "3. Demonstrate how to load the saved model and use it for making predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad160c9e1373b3fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:17:40.512385Z",
     "start_time": "2024-10-02T20:17:40.493064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict saved at mlp_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model’s state_dict (recommended)\n",
    "model_path = \"mlp_model.pth\"\n",
    "torch.save(model.state_dict(), model_path)\n",
    "print(f\"Model's state_dict saved at {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9146f5dd16930b67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:17:42.499585Z",
     "start_time": "2024-10-02T20:17:42.488065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire model saved at mlp_model_full.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model (includes architecture and weights)\n",
    "model_path_full = \"mlp_model_full.pth\"\n",
    "torch.save(model, model_path_full)\n",
    "print(f\"Entire model saved at {model_path_full}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b65cb2f47000ed68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:17:42.545293Z",
     "start_time": "2024-10-02T20:17:42.527190Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=5000, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To load the saved model for inference:\n",
    "# Loading the state_dict\n",
    "loaded_model = MLP(input_dim).to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_path))\n",
    "loaded_model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fad2d5294a91a521",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:17:44.126553Z",
     "start_time": "2024-10-02T20:17:44.116116Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (fc1): Linear(in_features=5000, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the entire model\n",
    "loaded_model_full = torch.load(model_path_full).to(device)\n",
    "loaded_model_full.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1504f7c692bd8c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T20:17:47.444530Z",
     "start_time": "2024-10-02T20:17:47.420768Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions on new data: tensor([[0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Example: Making predictions with the loaded model\n",
    "with torch.no_grad():\n",
    "    example_outputs = loaded_model(X_test_tensor[:5])\n",
    "    example_predictions = (example_outputs > 0.5).float()\n",
    "    print(f\"Predictions on new data: {example_predictions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b387d061cdd19d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# RNNs Tutorial with PyTorch on the IMDB Dataset\n",
    "\n",
    "In this tutorial, we will build a Recurrent Neural Network (CNN) to classify movie reviews as positive or negative using the IMDB dataset.\n",
    " \n",
    "We'll go through the following steps:\n",
    "\n",
    "1. **Data Loading, Processing, and Augmentation**\n",
    "2. **Data Exploration**\n",
    "3. **Model Building**\n",
    "4. **Model Training**\n",
    "5. **Model Evaluation**\n",
    "\n"
   ],
   "id": "9cf2cd67a7fb4ec1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Data Loading, Processing, and Augmentation\n",
    "\n",
    "The IMDB dataset contains 50,000 movie reviews, split into 25,000 reviews for training and 25,000 reviews for testing. The dataset is preprocessed, and each review is encoded as a sequence of word indexes.\n",
    "\n",
    "### Exercise 1.1 - Data Loading and Processing\n",
    "\n",
    "1. Load the data from `'../../data/imdb_dataset.zip'` using pandas.\n",
    "2. Convert the `sentiment` column to a binary label (0 for negative and 1 for positive).\n",
    "3. Split the dataset into training and test sets (25,000 samples each).\n",
    "4. Preprocess the `review` column by converting it to lowercase and removing special characters, punctuation and stopwords. Additionally, use a stemming or lemmatization technique.\n",
    "5. Build a vocabulary of unique words in the training dataset.\n",
    "6. Tokenize the reviews using the vocabulary, i.e., replace each word with its index in the vocabulary. Note that the reviews should be padded to a fixed length.\n",
    "7. Create the data loaders for training and test datasets.\n"
   ],
   "id": "de854ac82e9a020b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T12:06:08.756037Z",
     "start_time": "2024-11-08T12:06:07.669642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/tiago/dl_with_pytorch/data/imdb_dataset.zip\", compression='zip')\n",
    "\n",
    "print(df)"
   ],
   "id": "d98248f80f8d704a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  review sentiment\n",
      "0      One of the other reviewers has mentioned that ...  positive\n",
      "1      A wonderful little production. <br /><br />The...  positive\n",
      "2      I thought this was a wonderful way to spend ti...  positive\n",
      "3      Basically there's a family where a little boy ...  negative\n",
      "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
      "...                                                  ...       ...\n",
      "49995  I thought this movie did a down right good job...  positive\n",
      "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
      "49997  I am a Catholic taught in parochial elementary...  negative\n",
      "49998  I'm going to have to disagree with the previou...  negative\n",
      "49999  No one expects the Star Trek movies to be high...  negative\n",
      "\n",
      "[50000 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T15:59:52.302081Z",
     "start_time": "2024-11-13T15:59:52.086202Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "df.head()"
   ],
   "id": "4d35a16ffac61778",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mmap({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpositive\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m1\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnegative\u001B[39m\u001B[38;5;124m'\u001B[39m: \u001B[38;5;241m0\u001B[39m})\n\u001B[1;32m      2\u001B[0m df\u001B[38;5;241m.\u001B[39mhead()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-11-08T12:08:32.127111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(df, train_size=25000, test_size=25000, random_state=42)"
   ],
   "id": "b973103232079f75",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T12:06:08.774563Z",
     "start_time": "2024-11-08T12:06:08.769462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "2d5dc17df6189d98",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T12:06:08.781230Z",
     "start_time": "2024-11-08T12:06:08.774357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "ca686b162ef1e561",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T12:06:08.801804Z",
     "start_time": "2024-11-08T12:06:08.777664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "570e293bcd709a1f",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T12:06:08.802871Z",
     "start_time": "2024-11-08T12:06:08.780513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "116fc3f49260ae1c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T12:06:08.803367Z",
     "start_time": "2024-11-08T12:06:08.783645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "560106d252698ec9",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exercise 1.2 - Data Augmentation\n",
    "\n",
    "1. Define a function to replace random words with synonyms from WordNet in the reviews.\n",
    "2. Augment the training dataset by replacing words in a few reviews with synonyms.\n",
    "\n",
    "Note: Never augment the test dataset. It's essential to evaluate the model on the original data."
   ],
   "id": "7a2ad79060d1313f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T12:06:09.634773Z",
     "start_time": "2024-11-08T12:06:08.790118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def augment_text(text, max_augment=3):\n",
    "    tokens = text.split()[:20]\n",
    "    augmented_tokens = tokens.copy()\n",
    "    \n",
    "    for _ in range(max_augment):\n",
    "        if len(augmented_tokens) > 1:\n",
    "            rand_idx = random.randint(0, len(augmented_tokens) - 1)\n",
    "            word = augmented_tokens[rand_idx]\n",
    "            synonyms = wordnet.synsets(word)\n",
    "            if synonyms:\n",
    "                replacement = random.choice(synonyms).lemmas()[0].name()\n",
    "                augmented_tokens[rand_idx] = replacement\n",
    "    print(f\"Original: {' '.join(tokens)}\")\n",
    "    print(f\"Augmented: {' '.join(augmented_tokens)}\")\n",
    "    print('\\n\\n')\n",
    "    return ' '.join(augmented_tokens)\n",
    "\n",
    "# Apply augmentation to training set\n",
    "train_df['review'].iloc[:3].apply(lambda x: augment_text(x, max_augment=3))"
   ],
   "id": "cec1e57df2661d62",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/tiago/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 25\u001B[0m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(augmented_tokens)\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Apply augmentation to training set\u001B[39;00m\n\u001B[0;32m---> 25\u001B[0m \u001B[43mtrain_df\u001B[49m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreview\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39miloc[:\u001B[38;5;241m3\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: augment_text(x, max_augment\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. Data Exploration\n",
    "\n",
    "Before training our model, it's essential to explore the data. This helps us understand its distribution and visualize some examples.\n",
    "\n",
    "### Exercise 2 - Data Exploration\n",
    "1. Print the number of positive and negative reviews in the training and test datasets.\n",
    "2. Plot a histogram of the length of reviews in the training and test datasets.\n",
    "3. Print a few reviews and their corresponding sentiment labels."
   ],
   "id": "8c4d4b41cceb7666"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T12:06:09.649610Z",
     "start_time": "2024-11-08T12:06:09.636586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "3d3e17130dd15243",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-08T12:06:09.638226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# length of reviews (histogram)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ..."
   ],
   "id": "757af2ea7a33129d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-08T12:06:09.640996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "ba41e3c8301a24c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-08T12:06:09.644583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print reviews and sentiment labels\n",
    "# ..."
   ],
   "id": "c6b4941645567b17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Model Building\n",
    "\n",
    "Now, we'll define our RNN architecture. A typical RNN consists of an embedding layer, RNN layers, and a fully connected layers.\n",
    "\n",
    "### Exercise\n",
    "1. Define a RNN class inheriting from torch.nn.Module.\n",
    "2. Include one Embedding layer, two LSTM layers, and two Linear layers."
   ],
   "id": "ebf01dbc897fa0e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-08T12:06:09.647342Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "86bc9381447392ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Model Training\n",
    "\n",
    "We'll now define the training loop to optimize our model using the binary cross-entropy loss function and the Adam optimizer.\n",
    "\n",
    "### Exercise\n",
    "1. Define a function to train the model for a specified number of epochs.\n",
    "2. Print the training loss after each epoch."
   ],
   "id": "ef72a0844d23519e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T12:06:09.656137Z",
     "start_time": "2024-11-08T12:06:09.650256Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "60e9b2d2d4323664",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Model Evaluation\n",
    "\n",
    "After training, we need to evaluate our model on the test dataset to understand its performance.\n",
    "\n",
    "### Exercise\n",
    "1. Define a function to evaluate the model's accuracy on the test set.\n",
    "2. Print the accuracy."
   ],
   "id": "5ffdb68841ef1798"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-08T12:06:09.651804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ..."
   ],
   "id": "753013a29969972c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-08T12:06:09.653194Z"
    }
   },
   "cell_type": "code",
   "source": [],
   "id": "f97e3a9ed9ac1b32",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
